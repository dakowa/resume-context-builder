diff --git a/app_enhanced.py b/app_enhanced.py
new file mode 100644
index 0000000..b654241
--- /dev/null
+++ b/app_enhanced.py
@@ -0,0 +1,397 @@
+"""
+Resume Context Builder - Enhanced Streamlit App
+
+New features:
+- Multiple context database selection
+- Time range filtering for search
+- Information decay controls
+- Fixed file deletion
+"""
+
+import os
+import sys
+import base64
+import zipfile
+import uuid
+import time
+from pathlib import Path
+from datetime import datetime, timedelta
+
+import streamlit as st
+from context_packager_data.tokenizer import get_encoding
+import streamlit.components.v1 as components
+
+# Add parent directory to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from hr_tools.pdf_to_md import convert_pdfs_to_markdown
+from hr_tools.package_context import package_markdown_directory
+
+# Import enhanced modules
+from kb.db_enhanced import (
+    list_context_databases,
+    get_engine,
+    apply_information_decay,
+    get_decay_log,
+    get_time_range,
+    DEFAULT_CONTEXT_DB,
+)
+from kb.upsert_enhanced import (
+    upsert_markdown_files,
+    delete_file_from_context,
+    list_files_in_context,
+)
+from kb.search_enhanced import HybridSearcher
+
+
+st.set_page_config(
+    page_title="Resume Context Builder (Enhanced)",
+    layout="wide",
+    initial_sidebar_state="expanded"
+)
+
+st.title("Resume Context Builder")
+st.caption("Enhanced version with multiple contexts, time filtering, and information decay")
+
+# Initialize session state
+if "context_db" not in st.session_state:
+    st.session_state.context_db = DEFAULT_CONTEXT_DB
+
+# Get available contexts
+available_contexts = list_context_databases()
+if not available_contexts:
+    available_contexts = [DEFAULT_CONTEXT_DB]
+
+# Sidebar with enhanced options
+with st.sidebar:
+    st.header("Context Database")
+    
+    # Context selection
+    selected_context = st.selectbox(
+        "Select Context",
+        options=available_contexts,
+        index=available_contexts.index(st.session_state.context_db) if st.session_state.context_db in available_contexts else 0,
+        help="Choose which context database to use"
+    )
+    st.session_state.context_db = selected_context
+    
+    # Create new context
+    with st.expander("Create New Context"):
+        new_context_name = st.text_input("New context name", placeholder="e.g., project-alpha")
+        if st.button("Create Context") and new_context_name:
+            if new_context_name not in available_contexts:
+                get_engine(new_context_name)  # Initialize
+                st.success(f"Created context: {new_context_name}")
+                st.rerun()
+            else:
+                st.error("Context already exists")
+    
+    st.divider()
+    
+    # Information Decay Controls
+    st.header("Information Decay")
+    with st.expander("Auto-Cleanup Settings"):
+        decay_days = st.number_input(
+            "Remove data older than (days)",
+            min_value=1,
+            max_value=3650,
+            value=365,
+            help="Automatically remove data older than this many days"
+        )
+        
+        col1, col2 = st.columns(2)
+        with col1:
+            if st.button("Preview Decay", use_container_width=True):
+                engine = get_engine(st.session_state.context_db)
+                results = apply_information_decay(engine, max_age_days=int(decay_days), dry_run=True)
+                st.write("Would delete:")
+                st.write(f"- Chunks: {results['chunks']}")
+                st.write(f"- File entries: {results['file_index']}")
+                st.write(f"- Job runs: {results['job_runs']}")
+                st.write(f"- Feedback: {results['feedback']}")
+        
+        with col2:
+            if st.button("Apply Decay", type="primary", use_container_width=True):
+                engine = get_engine(st.session_state.context_db)
+                results = apply_information_decay(engine, max_age_days=int(decay_days), dry_run=False)
+                st.success(f"Deleted: {results['chunks']} chunks, {results['file_index']} files")
+    
+    st.divider()
+    
+    # Settings
+    st.header("Settings")
+    md_dir = st.text_input("Output Markdown directory", value=str(Path.home() / "context-packager-md"))
+    output_file = st.text_input("Packaged context output file", value=str(Path.home() / "output" / "context.md"))
+    use_hr_instructions = st.checkbox("Include HR instructions", value=True)
+    instruction_file = st.text_input("Instruction file path", value="")
+    header_text = st.text_input("Header text", value="HR Candidate Resume Pack")
+    max_tokens = st.number_input("Max tokens per file (0 = no split)", value=120000, min_value=0, step=1000)
+    encoding_name = st.selectbox("Tokenizer/encoding", options=["o200k_base", "cl100k_base"], index=0)
+
+# Main content area
+tab_ingest, tab_search, tab_manage = st.tabs(["üì§ Ingest", "üîç Search", "‚öôÔ∏è Manage"])
+
+# Tab 1: Ingest
+with tab_ingest:
+    st.subheader(f"Ingest into Context: `{st.session_state.context_db}`")
+    
+    uploaded_files = st.file_uploader(
+        "Upload ZIP or supported files",
+        type=None,
+        accept_multiple_files=True,
+        key="file_uploader"
+    )
+    fallback_dir_main = st.text_input("Or enter a directory path (optional)", value="")
+    
+    col1, col2, col3 = st.columns([1, 2, 1])
+    with col2:
+        start = st.button("Process and Package", type="primary", use_container_width=True)
+        clear = st.button("Reset", use_container_width=True)
+    
+    if clear:
+        st.session_state.pop("context_content", None)
+        st.session_state.pop("out_paths", None)
+        st.session_state.pop("selected_chunk", None)
+        try:
+            st.session_state["file_uploader"] = []
+        except Exception:
+            pass
+    
+    if start:
+        effective_pdf_dir = None
+        
+        if uploaded_files:
+            uploads_root = Path.home() / ".context-packager-uploads"
+            uploads_root.mkdir(parents=True, exist_ok=True)
+            temp_root = uploads_root / f"session-{uuid.uuid4().hex[:8]}"
+            temp_root.mkdir(parents=True, exist_ok=True)
+            combined_dir = temp_root / "combined"
+            combined_dir.mkdir(parents=True, exist_ok=True)
+            
+            for f in uploaded_files:
+                name = Path(f.name).name
+                if name.lower().endswith(".zip"):
+                    zip_path = temp_root / name
+                    zip_path.write_bytes(f.read())
+                    with zipfile.ZipFile(zip_path, "r") as zf:
+                        zf.extractall(combined_dir)
+                else:
+                    (combined_dir / name).write_bytes(f.read())
+            effective_pdf_dir = str(combined_dir.resolve())
+        elif fallback_dir_main and os.path.isdir(fallback_dir_main):
+            effective_pdf_dir = fallback_dir_main
+        else:
+            st.error("Please upload a ZIP/PDFs or provide a valid directory path.")
+            st.stop()
+        
+        Path(md_dir).mkdir(parents=True, exist_ok=True)
+        Path(Path(output_file).parent).mkdir(parents=True, exist_ok=True)
+        
+        with st.spinner("Converting PDFs to Markdown..."):
+            generated_md = convert_pdfs_to_markdown(effective_pdf_dir, md_dir)
+            st.success(f"Converted {len(generated_md)} markdown files.")
+        
+        # Upsert into selected context
+        with st.spinner(f"Upserting into context '{st.session_state.context_db}'..."):
+            count = upsert_markdown_files(
+                [Path(p) for p in generated_md],
+                context_db=st.session_state.context_db,
+                max_tokens_per_chunk=1500,
+                overlap_tokens=150,
+                encoding_name=encoding_name
+            )
+            st.success(f"Upserted {count} chunks into context '{st.session_state.context_db}'")
+        
+        with st.spinner("Packaging context with Repomix..."):
+            instr_path = instruction_file if (use_hr_instructions and os.path.isfile(instruction_file)) else None
+            run_tag = time.strftime("%Y%m%d-%H%M%S") + "-" + uuid.uuid4().hex[:6]
+            base = Path(output_file)
+            unique_output = base.with_name(f"{base.stem}_{run_tag}{base.suffix}")
+            out_paths = package_markdown_directory(
+                md_dir,
+                str(unique_output),
+                instruction_file=instr_path,
+                header_text=header_text,
+                max_tokens=max_tokens or None,
+                encoding_name=encoding_name,
+                predefined_md_files=[str(p) for p in generated_md],
+            )
+            st.success(f"Packaged {len(out_paths)} file(s)")
+            
+            try:
+                content = Path(out_paths[0]).read_text(encoding="utf-8")
+                st.session_state["context_content"] = content
+            except Exception as e:
+                st.error(f"Failed to read output file: {e}")
+            st.session_state["out_paths"] = [str(p) for p in out_paths]
+            
+            # Show token counts
+            enc = get_encoding(encoding_name)
+            rows = []
+            for p in out_paths:
+                text = Path(p).read_text(encoding="utf-8")
+                tok = len(enc.encode(text))
+                rows.append((str(p), tok))
+            st.write("Generated files (with token counts):")
+            for p, tok in rows:
+                st.write(f"{p} ‚Äî tokens: {tok}")
+        
+        try:
+            st.session_state["file_uploader"] = []
+        except Exception:
+            pass
+    
+    if "out_paths" in st.session_state and st.session_state["out_paths"]:
+        st.subheader("Packaged Context")
+        options = [f"Part {i+1}: {Path(p).name}" for i, p in enumerate(st.session_state["out_paths"])]
+        idx = st.session_state.get("selected_chunk", 0)
+        idx = st.selectbox("Select chunk", options=list(range(len(options))), format_func=lambda i: options[i], index=min(idx, len(options)-1))
+        st.session_state["selected_chunk"] = idx
+        selected_path = st.session_state["out_paths"][idx]
+        selected_content = Path(selected_path).read_text(encoding="utf-8")
+        
+        col_dl, col_cp = st.columns(2)
+        with col_dl:
+            st.download_button(
+                label=f"Download {Path(selected_path).name}",
+                data=selected_content,
+                file_name=Path(selected_path).name,
+                mime="text/markdown",
+                use_container_width=True,
+            )
+        with col_cp:
+            b64 = base64.b64encode(selected_content.encode("utf-8")).decode("ascii")
+            html = f"""
+            <style>
+            .copy-wrap {{ position: relative; width: 100%; }}
+            .copy-btn {{
+              all: unset; display: inline-block; width: 100%; text-align: center;
+              padding: 0.6rem 1rem; border-radius: 0.25rem; background-color: #F63366;
+              color: #fff; cursor: pointer; font-weight: 600;
+            }}
+            .copy-btn:hover {{ filter: brightness(0.95); }}
+            .copy-btn.copied {{ background-color: #22c55e; }}
+            </style>
+            <script>
+            async function copyChunk(){{
+              const text = atob('{b64}');
+              try {{ await navigator.clipboard.writeText(text); }} catch(e) {{
+                const ta = document.createElement('textarea'); ta.value = text; document.body.appendChild(ta);
+                ta.focus(); ta.select(); try {{ document.execCommand('copy'); }} catch(e2) {{}} document.body.removeChild(ta);
+              }}
+              const btn = document.getElementById('copy-btn');
+              if (btn) {{ btn.classList.add('copied'); btn.textContent = 'Copied!'; setTimeout(()=>{{ btn.classList.remove('copied'); btn.textContent = 'Copy to clipboard'; }}, 1200); }}
+            }}
+            </script>
+            <div class="copy-wrap">
+              <button id="copy-btn" class="copy-btn" onclick="copyChunk()">Copy to clipboard</button>
+            </div>
+            """
+            components.html(html, height=60)
+        
+        st.text_area("Preview", selected_content, height=400)
+
+# Tab 2: Search
+with tab_search:
+    st.subheader(f"Search Context: `{st.session_state.context_db}`")
+    
+    # Time range filter
+    col1, col2 = st.columns([3, 1])
+    with col1:
+        search_query = st.text_input("Search query", placeholder="Enter search terms...")
+    with col2:
+        time_range = st.selectbox(
+            "Time range",
+            options=["all", "1d", "1w", "1m", "3m", "6m", "1y"],
+            format_func=lambda x: {
+                "all": "All time",
+                "1d": "Last 1 day",
+                "1w": "Last 1 week",
+                "1m": "Last 1 month",
+                "3m": "Last 3 months",
+                "6m": "Last 6 months",
+                "1y": "Last 1 year"
+            }.get(x, x)
+        )
+    
+    top_k = st.slider("Number of results", min_value=1, max_value=50, value=10)
+    
+    if st.button("Search", type="primary"):
+        if not search_query:
+            st.warning("Please enter a search query")
+        else:
+            with st.spinner("Searching..."):
+                try:
+                    searcher = HybridSearcher(st.session_state.context_db)
+                    searcher.fit_from_db(time_range=time_range)
+                    results = searcher.search(search_query, top_k=top_k, time_range=time_range)
+                    
+                    if not results:
+                        st.info("No results found")
+                    else:
+                        st.success(f"Found {len(results)} results")
+                        for i, (cid, score, path, cname, snippet, full_text) in enumerate(results, 1):
+                            with st.expander(f"{i}. Score: {score:.4f} | {Path(path).name}"):
+                                st.write(f"**Chunk:** {cname}")
+                                st.write(f"**Path:** {path}")
+                                st.write(f"**Content:**")
+                                st.text(snippet[:1000] + ("..." if len(snippet) > 1000 else ""))
+                                
+                                # Copy button for this result
+                                b64 = base64.b64encode(full_text.encode("utf-8")).decode("ascii")
+                                html = f"""
+                                <script>
+                                async function copyResult{i}(){{
+                                  const text = atob('{b64}');
+                                  try {{ await navigator.clipboard.writeText(text); }} catch(e) {{
+                                    const ta = document.createElement('textarea'); ta.value = text; document.body.appendChild(ta);
+                                    ta.focus(); ta.select(); try {{ document.execCommand('copy'); }} catch(e2) {{}} document.body.removeChild(ta);
+                                  }}
+                                  alert('Copied to clipboard!');
+                                }}
+                                </script>
+                                <button onclick="copyResult{i}()" style="padding: 0.5rem 1rem; background: #F63366; color: white; border: none; border-radius: 0.25rem; cursor: pointer;">Copy full text</button>
+                                """
+                                components.html(html, height=50)
+                except Exception as e:
+                    st.error(f"Search failed: {e}")
+
+# Tab 3: Manage
+with tab_manage:
+    st.subheader(f"Manage Context: `{st.session_state.context_db}`")
+    
+    # List files in context
+    st.write("### Files in Context")
+    files = list_files_in_context(st.session_state.context_db)
+    
+    if not files:
+        st.info("No files in this context")
+    else:
+        st.write(f"Total files: {len(files)}")
+        
+        for path, sha, updated in files:
+            col1, col2 = st.columns([4, 1])
+            with col1:
+                st.write(f"- `{path}`")
+                st.caption(f"Updated: {updated}")
+            with col2:
+                if st.button("Delete", key=f"del_{sha}"):
+                    deleted = delete_file_from_context(path, st.session_state.context_db)
+                    st.success(f"Deleted {deleted} chunks")
+                    st.rerun()
+    
+    st.divider()
+    
+    # Decay log
+    st.write("### Decay History")
+    engine = get_engine(st.session_state.context_db)
+    logs = get_decay_log(engine, limit=5)
+    
+    if not logs:
+        st.info("No decay operations recorded")
+    else:
+        for action, details, executed_at in logs:
+            st.write(f"**{executed_at}**")
+            st.write(f"Action: {action}")
+            st.write(f"Details: {details}")
+            st.divider()
diff --git a/kb/create_model.py b/kb/create_model.py
new file mode 100644
index 0000000..ba89284
--- /dev/null
+++ b/kb/create_model.py
@@ -0,0 +1,59 @@
+import json
+import random
+from typing import Dict, Any
+
+def create_synthetic_model():
+    """
+    Generate a synthetic pretrained model that biases towards defaults.
+    """
+    # Defaults from tuning.py
+    DEFAULTS = {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": True,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1,
+    }
+    
+    # Root node with high visits and positive value
+    root = {
+        "params": DEFAULTS,
+        "visits": 100,
+        "value": 50.0,
+        "children": []
+    }
+    
+    # Add some variations as children
+    # 1. High recall (more neighbors, lower threshold)
+    v1 = DEFAULTS.copy()
+    v1["kb_top_k"] = 10
+    v1["kb_neighbors"] = 1
+    root["children"].append({
+        "params": v1,
+        "visits": 20,
+        "value": 5.0,
+        "children": []
+    })
+    
+    # 2. Precision (high bm25)
+    v2 = DEFAULTS.copy()
+    v2["kb_bm25_weight"] = 0.8
+    root["children"].append({
+        "params": v2,
+        "visits": 20,
+        "value": 2.0,
+        "children": []
+    })
+
+    with open("kb/tuning_model.json", "w", encoding="utf-8") as f:
+        json.dump(root, f, indent=2)
+    print("Created kb/tuning_model.json")
+
+if __name__ == "__main__":
+    create_synthetic_model()
+
diff --git a/kb/db.py b/kb/db.py
index d4f649d..bfab592 100644
--- a/kb/db.py
+++ b/kb/db.py
@@ -173,6 +173,41 @@ def init_schema(engine: Engine) -> None:
 				)
 		except Exception:
 			pass
+		# Feedback tracking for RL
+		conn.execute(
+			text(
+				"""
+				CREATE TABLE IF NOT EXISTS feedback (
+					id INTEGER PRIMARY KEY AUTOINCREMENT,
+					query TEXT,
+					chunk_id INTEGER,
+					score REAL,
+					created_at TEXT
+				)
+				"""
+			)
+		)
+		conn.execute(
+			text(
+				"""
+				CREATE INDEX IF NOT EXISTS idx_feedback_query ON feedback(query)
+				"""
+			)
+		)
+		# Search history for tuning/eval
+		conn.execute(
+			text(
+				"""
+				CREATE TABLE IF NOT EXISTS search_history (
+					id INTEGER PRIMARY KEY AUTOINCREMENT,
+					query TEXT,
+					params_json TEXT,
+					result_count INTEGER,
+					created_at TEXT
+				)
+				"""
+			)
+		)
 
 
 def ensure_job(job_id: str, name: str | None, input_dir: str, md_out_dir: str | None, sla_minutes: int | None = None) -> None:
@@ -671,3 +706,59 @@ def factory_reset_db() -> None:
 		# Best-effort reset; swallow exceptions to avoid breaking UI
 		return
 
+
+def record_feedback(query: str, chunk_id: int, score: float) -> None:
+	"""Record user feedback (thumbs up=1.0, down=-1.0) for a query result."""
+	engine = get_engine()
+	now = datetime.utcnow().isoformat()
+	with engine.begin() as conn:
+		conn.execute(
+			text("INSERT INTO feedback(query, chunk_id, score, created_at) VALUES(:q, :cid, :s, :now)"),
+			{"q": query.strip(), "cid": chunk_id, "s": score, "now": now}
+		)
+
+
+def fetch_feedback_for_query(query: str) -> List[Tuple[int, float]]:
+	"""Return list of (chunk_id, score) for a query."""
+	engine = get_engine()
+	with engine.begin() as conn:
+		rows = conn.execute(
+			text("SELECT chunk_id, score FROM feedback WHERE query=:q"),
+			{"q": query.strip()}
+		).fetchall()
+		return [(r[0], r[1]) for r in rows]
+
+def record_search_history(query: str, params: dict, count: int) -> int:
+	"""Log a search event and its parameters."""
+	import json
+	engine = get_engine()
+	now = datetime.utcnow().isoformat()
+	with engine.begin() as conn:
+		res = conn.execute(
+			text("INSERT INTO search_history(query, params_json, result_count, created_at) VALUES(:q, :p, :c, :now)"),
+			{"q": query, "p": json.dumps(params), "c": count, "now": now}
+		)
+		if res.lastrowid:
+			return int(res.lastrowid)
+		return 0
+
+def fetch_search_history(limit: int = 100) -> List[Dict]:
+	"""Return recent searches for tuning analysis."""
+	import json
+	engine = get_engine()
+	with engine.begin() as conn:
+		rows = conn.execute(
+			text("SELECT id, query, params_json, result_count, created_at FROM search_history ORDER BY id DESC LIMIT :limit"),
+			{"limit": limit}
+		)
+		out = []
+		for r in rows:
+			try:
+				params = json.loads(r[2])
+			except Exception:
+				params = {}
+			out.append({
+				"id": r[0], "query": r[1], "params": params, "count": r[3], "created_at": r[4]
+			})
+		return out
+
diff --git a/kb/db_enhanced.py b/kb/db_enhanced.py
new file mode 100644
index 0000000..c41e990
--- /dev/null
+++ b/kb/db_enhanced.py
@@ -0,0 +1,1170 @@
+"""
+Enhanced Database Module for Resume Context Builder
+
+Features:
+- Multiple context databases support
+- Information decay (auto-remove old data)
+- Time-based filtering
+- Fixed deletion functionality
+"""
+
+import os
+import hashlib
+from datetime import datetime, timedelta
+from pathlib import Path
+from typing import Iterable, List, Tuple, Dict, Optional, Any
+
+from sqlalchemy import create_engine, text, Engine
+import uuid
+
+# State directory for all context databases
+STATE_DIR = Path.home() / ".context-packager-state"
+STATE_DIR.mkdir(parents=True, exist_ok=True)
+
+# Default context database
+DEFAULT_CONTEXT_DB = "default"
+
+
+def get_database_url(context_db: str = DEFAULT_CONTEXT_DB) -> str:
+    """Get database URL for a specific context database.
+    
+    Args:
+        context_db: Name of the context database (default: "default")
+    """
+    # Prefer explicit env vars; fallback to stateful SQLite file
+    url = (
+        os.getenv("CONTEXT_DB_URL")
+        or os.getenv("DATABASE_URL")
+        or os.getenv("PGSERVER_URL")
+        or os.getenv("PGSERVER")
+    )
+    if url:
+        return url
+    
+    # Use separate database file for each context
+    db_file = STATE_DIR / f"context_{context_db}.db"
+    return f"sqlite:///{db_file.as_posix()}"
+
+
+def get_engine(context_db: str = DEFAULT_CONTEXT_DB, echo: bool = False) -> Engine:
+    """Get SQLAlchemy engine for a specific context database."""
+    engine = create_engine(
+        get_database_url(context_db), 
+        echo=echo, 
+        future=True, 
+        pool_pre_ping=True
+    )
+    init_schema(engine)
+    return engine
+
+
+def list_context_databases() -> List[str]:
+    """List all available context databases."""
+    databases = []
+    for db_file in STATE_DIR.glob("context_*.db"):
+        # Extract context name from filename (context_NAME.db)
+        name = db_file.stem.replace("context_", "")
+        if name:
+            databases.append(name)
+    return sorted(databases)
+
+
+def delete_context_database(context_db: str) -> bool:
+    """Delete a context database entirely.
+    
+    Returns True if deleted, False if not found or is default.
+    """
+    if context_db == DEFAULT_CONTEXT_DB:
+        return False  # Protect default database
+    
+    db_file = STATE_DIR / f"context_{context_db}.db"
+    if db_file.exists():
+        try:
+            db_file.unlink()
+            return True
+        except Exception:
+            pass
+    return False
+
+
+def init_schema(engine: Engine) -> None:
+    """Initialize database schema with enhanced tables."""
+    with engine.begin() as conn:
+        # Chunks table with context support and timestamps
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS chunks (
+                    id INTEGER PRIMARY KEY AUTOINCREMENT,
+                    path TEXT,
+                    chunk_name TEXT,
+                    hash TEXT UNIQUE,
+                    content TEXT,
+                    created_at TEXT,
+                    updated_at TEXT,
+                    source_date TEXT  -- Original document date if available
+                )
+                """
+            )
+        )
+        conn.execute(
+            text(
+                """
+                CREATE INDEX IF NOT EXISTS idx_chunks_hash ON chunks(hash)
+                """
+            )
+        )
+        conn.execute(
+            text(
+                """
+                CREATE INDEX IF NOT EXISTS idx_chunks_created_at ON chunks(created_at)
+                """
+            )
+        )
+        conn.execute(
+            text(
+                """
+                CREATE INDEX IF NOT EXISTS idx_chunks_source_date ON chunks(source_date)
+                """
+            )
+        )
+        
+        # Meta nodes table
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS meta_nodes (
+                    id INTEGER PRIMARY KEY AUTOINCREMENT,
+                    type TEXT,
+                    name TEXT,
+                    normalized_name TEXT UNIQUE
+                )
+                """
+            )
+        )
+        
+        # Chunk-meta edges
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS chunk_meta_edges (
+                    chunk_id INTEGER,
+                    meta_node_id INTEGER,
+                    weight REAL,
+                    PRIMARY KEY (chunk_id, meta_node_id)
+                )
+                """
+            )
+        )
+        
+        # File index with enhanced tracking
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS file_index (
+                    path TEXT PRIMARY KEY,
+                    sha256 TEXT,
+                    params_sig TEXT,
+                    updated_at TEXT,
+                    file_date TEXT  -- Original file modification date
+                )
+                """
+            )
+        )
+        
+        # Jobs metadata
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS jobs (
+                    id TEXT PRIMARY KEY,
+                    name TEXT,
+                    input_dir TEXT,
+                    md_out_dir TEXT,
+                    sla_minutes INTEGER,
+                    created_at TEXT,
+                    updated_at TEXT
+                )
+                """
+            )
+        )
+        
+        # Backfill migration: ensure sla_minutes column exists
+        try:
+            if engine.dialect.name == "sqlite":
+                rows = conn.execute(text("PRAGMA table_info(jobs)")).fetchall()
+                cols = [r[1] for r in rows]
+                if "sla_minutes" not in cols:
+                    conn.execute(text("ALTER TABLE jobs ADD COLUMN sla_minutes INTEGER"))
+            else:
+                conn.execute(text("ALTER TABLE jobs ADD COLUMN IF NOT EXISTS sla_minutes INTEGER"))
+        except Exception:
+            pass
+        
+        # Job runs with enhanced tracking
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS job_runs (
+                    id TEXT PRIMARY KEY,
+                    job_id TEXT,
+                    input_dir TEXT,
+                    md_out_dir TEXT,
+                    status TEXT,
+                    progress INTEGER,
+                    processed_files INTEGER,
+                    total_files INTEGER,
+                    chunks_upserted INTEGER,
+                    started_at TEXT,
+                    finished_at TEXT,
+                    last_message TEXT,
+                    log TEXT,
+                    cancel_requested INTEGER DEFAULT 0,
+                    error TEXT
+                )
+                """
+            )
+        )
+        conn.execute(
+            text(
+                """
+                CREATE INDEX IF NOT EXISTS idx_job_runs_started_at ON job_runs(started_at)
+                """
+            )
+        )
+        
+        # Feedback tracking
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS feedback (
+                    id INTEGER PRIMARY KEY AUTOINCREMENT,
+                    query TEXT,
+                    chunk_id INTEGER,
+                    score REAL,
+                    created_at TEXT
+                )
+                """
+            )
+        )
+        conn.execute(
+            text(
+                """
+                CREATE INDEX IF NOT EXISTS idx_feedback_query ON feedback(query)
+                """
+            )
+        )
+        
+        # Search history
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS search_history (
+                    id INTEGER PRIMARY KEY AUTOINCREMENT,
+                    query TEXT,
+                    params_json TEXT,
+                    result_count INTEGER,
+                    created_at TEXT
+                )
+                """
+            )
+        )
+        
+        # Decay tracking table
+        conn.execute(
+            text(
+                """
+                CREATE TABLE IF NOT EXISTS decay_log (
+                    id INTEGER PRIMARY KEY AUTOINCREMENT,
+                    action TEXT,
+                    details TEXT,
+                    executed_at TEXT
+                )
+                """
+            )
+        )
+        
+        # Prepare pgvector schema if available
+        try:
+            if engine.dialect.name == "postgresql":
+                conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
+                conn.execute(
+                    text(
+                        """
+                        CREATE TABLE IF NOT EXISTS chunk_vectors (
+                            chunk_id BIGINT PRIMARY KEY,
+                            svd vector,
+                            updated_at TIMESTAMP
+                        )
+                        """
+                    )
+                )
+        except Exception:
+            pass
+
+
+# ============================================================================
+# Context Database Management
+# ============================================================================
+
+def ensure_job(
+    job_id: str, 
+    name: str | None, 
+    input_dir: str, 
+    md_out_dir: str | None, 
+    sla_minutes: int | None = None,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> None:
+    """Upsert a job metadata row for visibility in UI."""
+    engine = get_engine(context_db)
+    now = datetime.utcnow().isoformat()
+    with engine.begin() as conn:
+        conn.execute(
+            text(
+                """
+                INSERT INTO jobs(id, name, input_dir, md_out_dir, sla_minutes, created_at, updated_at)
+                VALUES(:id, :name, :in_dir, :out_dir, :sla, :now, :now)
+                ON CONFLICT(id) DO UPDATE SET
+                    name=excluded.name,
+                    input_dir=excluded.input_dir,
+                    md_out_dir=excluded.md_out_dir,
+                    sla_minutes=excluded.sla_minutes,
+                    updated_at=excluded.updated_at
+                """
+            ),
+            {
+                "id": job_id, 
+                "name": name, 
+                "in_dir": input_dir, 
+                "out_dir": md_out_dir, 
+                "sla": (int(sla_minutes) if sla_minutes is not None else None), 
+                "now": now
+            },
+        )
+
+
+def start_job_run(
+    job_id: str | None, 
+    input_dir: str, 
+    md_out_dir: str | None,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> str:
+    """Create a job_run row and return its id."""
+    engine = get_engine(context_db)
+    run_id = uuid.uuid4().hex
+    now = datetime.utcnow().isoformat()
+    with engine.begin() as conn:
+        conn.execute(
+            text(
+                """
+                INSERT INTO job_runs(id, job_id, input_dir, md_out_dir, status, progress, processed_files, total_files, chunks_upserted, started_at, finished_at, last_message, log, cancel_requested, error)
+                VALUES(:id, :job_id, :in_dir, :out_dir, 'running', 0, 0, 0, 0, :now, NULL, '', '', 0, NULL)
+                """
+            ),
+            {"id": run_id, "job_id": job_id, "in_dir": input_dir, "out_dir": md_out_dir, "now": now},
+        )
+    return run_id
+
+
+def update_job_run(
+    run_id: str,
+    *,
+    status: str | None = None,
+    progress: int | None = None,
+    processed_files: int | None = None,
+    total_files: int | None = None,
+    chunks_upserted: int | None = None,
+    last_message: str | None = None,
+    append_log: str | None = None,
+    error: str | None = None,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> None:
+    """Partial update of a job_run row."""
+    engine = get_engine(context_db)
+    sets: list[str] = []
+    params: dict = {"id": run_id}
+    if status is not None:
+        sets.append("status=:status")
+        params["status"] = status
+    if progress is not None:
+        sets.append("progress=:progress")
+        params["progress"] = int(max(0, min(100, progress)))
+    if processed_files is not None:
+        sets.append("processed_files=:pf")
+        params["pf"] = int(processed_files)
+    if total_files is not None:
+        sets.append("total_files=:tf")
+        params["tf"] = int(total_files)
+    if chunks_upserted is not None:
+        sets.append("chunks_upserted=:cu")
+        params["cu"] = int(chunks_upserted)
+    if last_message is not None:
+        sets.append("last_message=:lm")
+        params["lm"] = str(last_message)
+    if error is not None:
+        sets.append("error=:err")
+        params["err"] = str(error)
+    stmt = "UPDATE job_runs SET " + ", ".join(sets) + " WHERE id=:id"
+    if sets:
+        with engine.begin() as conn:
+            conn.execute(text(stmt), params)
+    if append_log:
+        with engine.begin() as conn:
+            conn.execute(
+                text(
+                    """
+                    UPDATE job_runs
+                    SET log=COALESCE(log,'') || :chunk
+                    WHERE id=:id
+                    """
+                ),
+                {"id": run_id, "chunk": (append_log or "")},
+            )
+
+
+def finish_job_run(
+    run_id: str, 
+    *, 
+    status: str, 
+    error: str | None = None,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> None:
+    """Mark a run finished and set status and optional error."""
+    engine = get_engine(context_db)
+    now = datetime.utcnow().isoformat()
+    with engine.begin() as conn:
+        conn.execute(
+            text(
+                """
+                UPDATE job_runs
+                SET status=:status, finished_at=:now, error=:error
+                WHERE id=:id
+                """
+            ),
+            {"id": run_id, "status": status, "now": now, "error": error},
+        )
+
+
+def request_cancel_run(run_id: str, context_db: str = DEFAULT_CONTEXT_DB) -> None:
+    """Signal a running job to cancel."""
+    engine = get_engine(context_db)
+    with engine.begin() as conn:
+        conn.execute(text("UPDATE job_runs SET cancel_requested=1 WHERE id=:id"), {"id": run_id})
+
+
+def is_cancel_requested(run_id: str, context_db: str = DEFAULT_CONTEXT_DB) -> bool:
+    engine = get_engine(context_db)
+    with engine.begin() as conn:
+        row = conn.execute(
+            text("SELECT cancel_requested FROM job_runs WHERE id=:id"), 
+            {"id": run_id}
+        ).fetchone()
+        try:
+            return bool(row[0]) if row else False
+        except Exception:
+            return False
+
+
+def fetch_recent_runs(
+    limit: int = 50, 
+    job_id: str | None = None,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> List[Tuple]:
+    """Return recent job runs, newest first."""
+    engine = get_engine(context_db)
+    params: dict = {"limit": int(limit)}
+    query = """SELECT id, job_id, input_dir, md_out_dir, status, progress, processed_files, total_files, chunks_upserted, started_at, finished_at, last_message, substr(log, -5000), cancel_requested, error FROM job_runs"""
+    if job_id:
+        query += " WHERE job_id=:job_id"
+        params["job_id"] = job_id
+    query += " ORDER BY started_at DESC LIMIT :limit"
+    with engine.begin() as conn:
+        rows = conn.execute(text(query), params)
+        return [tuple(r) for r in rows]
+
+
+def fetch_jobs(context_db: str = DEFAULT_CONTEXT_DB) -> List[Tuple]:
+    """Return jobs list."""
+    engine = get_engine(context_db)
+    with engine.begin() as conn:
+        rows = conn.execute(
+            text("SELECT id, name, input_dir, md_out_dir, created_at, updated_at, sla_minutes FROM jobs ORDER BY updated_at DESC")
+        )
+        return [tuple(r) for r in rows]
+
+
+def fetch_last_success(job_id: str, context_db: str = DEFAULT_CONTEXT_DB) -> str | None:
+    """Return ISO timestamp of last successful run for a job."""
+    engine = get_engine(context_db)
+    with engine.begin() as conn:
+        row = conn.execute(
+            text("SELECT started_at FROM job_runs WHERE job_id=:jid AND status='success' ORDER BY started_at DESC LIMIT 1"),
+            {"jid": job_id}
+        ).fetchone()
+        return str(row[0]) if row else None
+
+
+# ============================================================================
+# Chunk Operations
+# ============================================================================
+
+def compute_hash(content: str) -> str:
+    return hashlib.sha256(content.encode("utf-8")).hexdigest()
+
+
+def upsert_chunks(
+    engine: Engine, 
+    records: List[Tuple[str, str, str]],
+    source_date: str | None = None
+) -> None:
+    """Upsert chunk records with optional source date.
+    
+    records items: (path, chunk_name, content)
+    """
+    now = datetime.utcnow().isoformat()
+    with engine.begin() as conn:
+        for path, chunk_name, content in records:
+            h = compute_hash(content)
+            conn.execute(
+                text(
+                    """
+                    INSERT INTO chunks(path, chunk_name, hash, content, created_at, updated_at, source_date)
+                    VALUES(:path, :chunk_name, :hash, :content, :now, :now, :source_date)
+                    ON CONFLICT(hash) DO UPDATE SET
+                        path=excluded.path,
+                        chunk_name=excluded.chunk_name,
+                        content=excluded.content,
+                        updated_at=excluded.updated_at,
+                        source_date=COALESCE(excluded.source_date, chunks.source_date)
+                    """
+                ),
+                {
+                    "path": path, 
+                    "chunk_name": chunk_name, 
+                    "hash": h, 
+                    "content": content, 
+                    "now": now,
+                    "source_date": source_date
+                },
+            )
+
+
+def get_file_index(
+    engine: Engine, 
+    path: str
+) -> Tuple[str, str] | None:
+    """Return (sha256, params_sig) for a path if recorded."""
+    with engine.begin() as conn:
+        row = conn.execute(
+            text("SELECT sha256, params_sig FROM file_index WHERE path=:p"), 
+            {"p": path}
+        ).fetchone()
+        return (row[0], row[1]) if row else None
+
+
+def upsert_file_index(
+    engine: Engine, 
+    path: str, 
+    sha256: str, 
+    params_sig: str | None,
+    file_date: str | None = None
+) -> None:
+    now = datetime.utcnow().isoformat()
+    with engine.begin() as conn:
+        conn.execute(
+            text(
+                """
+                INSERT INTO file_index(path, sha256, params_sig, updated_at, file_date)
+                VALUES(:path, :sha, :psig, :now, :file_date)
+                ON CONFLICT(path) DO UPDATE SET
+                    sha256=excluded.sha256,
+                    params_sig=excluded.params_sig,
+                    updated_at=excluded.updated_at,
+                    file_date=COALESCE(excluded.file_date, file_index.file_date)
+                """
+            ),
+            {"path": path, "sha": sha256, "psig": params_sig, "now": now, "file_date": file_date},
+        )
+
+
+def delete_chunks_by_path(
+    engine: Engine, 
+    path: str
+) -> int:
+    """Delete all chunks that belong to a specific file path.
+    
+    Returns number of rows deleted.
+    """
+    with engine.begin() as conn:
+        # First get the count
+        count_result = conn.execute(
+            text("SELECT COUNT(*) FROM chunks WHERE path=:p"), 
+            {"p": path}
+        ).fetchone()
+        count = count_result[0] if count_result else 0
+        
+        # Then delete
+        conn.execute(text("DELETE FROM chunks WHERE path=:p"), {"p": path})
+        return count
+
+
+def delete_file_index(
+    engine: Engine, 
+    path: str
+) -> None:
+    """Remove file from file_index."""
+    with engine.begin() as conn:
+        conn.execute(text("DELETE FROM file_index WHERE path=:p"), {"p": path})
+
+
+def fetch_chunk_ids_by_path(
+    engine: Engine, 
+    path: str
+) -> List[int]:
+    """Return chunk IDs for a given file path."""
+    with engine.begin() as conn:
+        rows = conn.execute(
+            text("SELECT id FROM chunks WHERE path=:p"), 
+            {"p": path}
+        )
+        return [int(r[0]) for r in rows]
+
+
+def fetch_all_chunks(
+    engine: Engine,
+    since: str | None = None,
+    until: str | None = None
+) -> List[Tuple[int, str, str, str]]:
+    """Return list of (id, path, chunk_name, content) with optional time filtering.
+    
+    Args:
+        since: ISO datetime string - only return chunks created after this date
+        until: ISO datetime string - only return chunks created before this date
+    """
+    query = "SELECT id, path, chunk_name, content FROM chunks WHERE 1=1"
+    params: Dict[str, Any] = {}
+    
+    if since:
+        query += " AND created_at >= :since"
+        params["since"] = since
+    if until:
+        query += " AND created_at <= :until"
+        params["until"] = until
+    
+    with engine.begin() as conn:
+        rows = conn.execute(text(query), params)
+        return [(r[0], r[1], r[2], r[3]) for r in rows]
+
+
+def count_chunks(
+    engine: Engine, 
+    like: str | None = None,
+    since: str | None = None,
+    until: str | None = None
+) -> int:
+    """Return total number of chunks with optional filters."""
+    query = "SELECT COUNT(1) FROM chunks WHERE 1=1"
+    params: Dict[str, Any] = {}
+    
+    if like:
+        query += " AND (path LIKE :like OR chunk_name LIKE :like OR content LIKE :like)"
+        params["like"] = like
+    if since:
+        query += " AND created_at >= :since"
+        params["since"] = since
+    if until:
+        query += " AND created_at <= :until"
+        params["until"] = until
+    
+    with engine.begin() as conn:
+        row = conn.execute(text(query), params).fetchone()
+        return int(row[0]) if row else 0
+
+
+def fetch_chunks(
+    engine: Engine, 
+    limit: int = 50, 
+    offset: int = 0, 
+    like: str | None = None,
+    since: str | None = None,
+    until: str | None = None
+) -> List[Tuple[int, str, str, str]]:
+    """Paginated fetch with optional time filtering."""
+    base = "SELECT id, path, chunk_name, content FROM chunks WHERE 1=1"
+    params: Dict[str, Any] = {"limit": int(limit), "offset": int(offset)}
+    
+    if like:
+        base += " AND (path LIKE :like OR chunk_name LIKE :like OR content LIKE :like)"
+        params["like"] = like
+    if since:
+        base += " AND created_at >= :since"
+        params["since"] = since
+    if until:
+        base += " AND created_at <= :until"
+        params["until"] = until
+    
+    base += " ORDER BY id DESC LIMIT :limit OFFSET :offset"
+    
+    with engine.begin() as conn:
+        rows = conn.execute(text(base), params)
+        return [(r[0], r[1], r[2], r[3]) for r in rows]
+
+
+def delete_chunks_by_ids(
+    engine: Engine, 
+    ids: List[int]
+) -> int:
+    """Delete chunks by id list. Returns number of rows deleted."""
+    if not ids:
+        return 0
+    safe_ids = []
+    for i in ids:
+        try:
+            safe_ids.append(int(i))
+        except Exception:
+            continue
+    if not safe_ids:
+        return 0
+    placeholders = ", ".join(str(i) for i in sorted(set(safe_ids)))
+    with engine.begin() as conn:
+        res = conn.execute(text(f"DELETE FROM chunks WHERE id IN ({placeholders})"))
+        try:
+            return int(res.rowcount)
+        except Exception:
+            return 0
+
+
+def fetch_chunk_by_id(
+    engine: Engine, 
+    chunk_id: int
+) -> Tuple[int, str, str, str] | None:
+    """Fetch a single chunk by id."""
+    with engine.begin() as conn:
+        row = conn.execute(
+            text("SELECT id, path, chunk_name, content FROM chunks WHERE id=:id"), 
+            {"id": int(chunk_id)}
+        ).fetchone()
+        return (row[0], row[1], row[2], row[3]) if row else None
+
+
+# ============================================================================
+# Information Decay System
+# ============================================================================
+
+def apply_information_decay(
+    engine: Engine,
+    max_age_days: int = 365,
+    dry_run: bool = False
+) -> Dict[str, int]:
+    """Automatically remove data older than specified age.
+    
+    Args:
+        engine: Database engine
+        max_age_days: Maximum age in days (default: 365 = 1 year)
+        dry_run: If True, only count what would be deleted without actually deleting
+    
+    Returns:
+        Dict with counts of deleted items by category
+    """
+    cutoff_date = (datetime.utcnow() - timedelta(days=max_age_days)).isoformat()
+    results = {"chunks": 0, "file_index": 0, "job_runs": 0, "feedback": 0}
+    
+    with engine.begin() as conn:
+        # Count chunks to delete
+        count_result = conn.execute(
+            text("SELECT COUNT(*) FROM chunks WHERE created_at < :cutoff"),
+            {"cutoff": cutoff_date}
+        ).fetchone()
+        results["chunks"] = count_result[0] if count_result else 0
+        
+        # Count file_index entries to delete
+        count_result = conn.execute(
+            text("SELECT COUNT(*) FROM file_index WHERE updated_at < :cutoff"),
+            {"cutoff": cutoff_date}
+        ).fetchone()
+        results["file_index"] = count_result[0] if count_result else 0
+        
+        # Count old job_runs
+        count_result = conn.execute(
+            text("SELECT COUNT(*) FROM job_runs WHERE started_at < :cutoff"),
+            {"cutoff": cutoff_date}
+        ).fetchone()
+        results["job_runs"] = count_result[0] if count_result else 0
+        
+        # Count old feedback
+        count_result = conn.execute(
+            text("SELECT COUNT(*) FROM feedback WHERE created_at < :cutoff"),
+            {"cutoff": cutoff_date}
+        ).fetchone()
+        results["feedback"] = count_result[0] if count_result else 0
+        
+        if not dry_run:
+            # Actually delete
+            conn.execute(text("DELETE FROM chunks WHERE created_at < :cutoff"), {"cutoff": cutoff_date})
+            conn.execute(text("DELETE FROM file_index WHERE updated_at < :cutoff"), {"cutoff": cutoff_date})
+            conn.execute(text("DELETE FROM job_runs WHERE started_at < :cutoff"), {"cutoff": cutoff_date})
+            conn.execute(text("DELETE FROM feedback WHERE created_at < :cutoff"), {"cutoff": cutoff_date})
+            
+            # Log the decay action
+            conn.execute(
+                text("INSERT INTO decay_log(action, details, executed_at) VALUES(:action, :details, :now)"),
+                {
+                    "action": "decay_cleanup",
+                    "details": f"Deleted chunks:{results['chunks']}, files:{results['file_index']}, jobs:{results['job_runs']}, feedback:{results['feedback']}",
+                    "now": datetime.utcnow().isoformat()
+                }
+            )
+    
+    return results
+
+
+def get_decay_log(
+    engine: Engine,
+    limit: int = 10
+) -> List[Tuple[str, str, str]]:
+    """Get recent decay operations."""
+    with engine.begin() as conn:
+        rows = conn.execute(
+            text("SELECT action, details, executed_at FROM decay_log ORDER BY executed_at DESC LIMIT :limit"),
+            {"limit": limit}
+        )
+        return [(r[0], r[1], r[2]) for r in rows]
+
+
+# ============================================================================
+# Time Range Utilities
+# ============================================================================
+
+def get_time_range(
+    range_str: str
+) -> Tuple[str | None, str | None]:
+    """Convert a time range string to ISO datetime strings.
+    
+    Args:
+        range_str: One of:
+            - "1d" or "1day" - last 1 day
+            - "1w" or "1week" - last 1 week
+            - "1m" or "1month" - last 1 month
+            - "3m" or "3months" - last 3 months
+            - "6m" or "6months" - last 6 months
+            - "1y" or "1year" - last 1 year
+            - "all" - no time limit
+    
+    Returns:
+        Tuple of (since, until) - until is None for all ranges (means up to now)
+    """
+    now = datetime.utcnow()
+    until = None  # Up to now
+    
+    range_str = range_str.lower().strip()
+    
+    if range_str in ("all", "everything", "*"):
+        return (None, None)
+    
+    # Parse number and unit
+    import re
+    match = re.match(r'(\d+)\s*([a-z]+)', range_str)
+    if not match:
+        return (None, None)
+    
+    num = int(match.group(1))
+    unit = match.group(2)
+    
+    if unit in ('d', 'day', 'days'):
+        since = (now - timedelta(days=num)).isoformat()
+    elif unit in ('w', 'week', 'weeks'):
+        since = (now - timedelta(weeks=num)).isoformat()
+    elif unit in ('m', 'month', 'months'):
+        since = (now - timedelta(days=num * 30)).isoformat()  # Approximate
+    elif unit in ('y', 'year', 'years'):
+        since = (now - timedelta(days=num * 365)).isoformat()
+    else:
+        return (None, None)
+    
+    return (since, until)
+
+
+# ============================================================================
+# Meta Node Operations
+# ============================================================================
+
+def upsert_meta_node(
+    engine: Engine, 
+    node_type: str, 
+    name: str
+) -> int:
+    """Upsert a meta node and return its ID."""
+    norm = f"{node_type}::{name.strip().lower()}"
+    with engine.begin() as conn:
+        row = conn.execute(
+            text("SELECT id FROM meta_nodes WHERE normalized_name=:n"), 
+            {"n": norm}
+        ).fetchone()
+        if row:
+            return int(row[0])
+        
+        res = conn.execute(
+            text("INSERT INTO meta_nodes(type, name, normalized_name) VALUES(:t, :n, :nn)"),
+            {"t": node_type, "n": name.strip(), "nn": norm}
+        )
+        if res.lastrowid:
+            return int(res.lastrowid)
+        row = conn.execute(
+            text("SELECT id FROM meta_nodes WHERE normalized_name=:n"), 
+            {"n": norm}
+        ).fetchone()
+        return int(row[0])
+
+
+def link_chunk_to_meta(
+    engine: Engine, 
+    chunk_id: int, 
+    meta_id: int, 
+    weight: float = 1.0
+) -> None:
+    """Link a chunk to a meta node."""
+    with engine.begin() as conn:
+        conn.execute(
+            text(
+                """
+                INSERT INTO chunk_meta_edges(chunk_id, meta_node_id, weight)
+                VALUES(:cid, :mid, :w)
+                ON CONFLICT(chunk_id, meta_node_id) DO UPDATE SET weight=excluded.weight
+                """
+            ),
+            {"cid": chunk_id, "mid": meta_id, "w": weight}
+        )
+
+
+def fetch_meta_nodes_for_chunks(
+    engine: Engine, 
+    chunk_ids: List[int]
+) -> Dict[int, List[Tuple[str, str]]]:
+    """Return {chunk_id: [(type, name), ...]}."""
+    if not chunk_ids:
+        return {}
+    ids_str = ",".join(str(i) for i in chunk_ids)
+    with engine.begin() as conn:
+        rows = conn.execute(
+            text(
+                f"""
+                SELECT e.chunk_id, m.type, m.name
+                FROM chunk_meta_edges e
+                JOIN meta_nodes m ON e.meta_node_id = m.id
+                WHERE e.chunk_id IN ({ids_str})
+                """
+            )
+        )
+        res = {}
+        for cid, t, n in rows:
+            res.setdefault(cid, []).append((t, n))
+        return res
+
+
+# ============================================================================
+# pgvector Operations (Postgres only)
+# ============================================================================
+
+def persist_chunk_vectors(
+    engine: Engine, 
+    rows: List[Tuple[int, List[float]]]
+) -> int:
+    """Persist SVD vectors to Postgres pgvector table."""
+    if not rows:
+        return 0
+    if engine.dialect.name != "postgresql":
+        return 0
+    
+    with engine.begin() as conn:
+        try:
+            conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
+            conn.execute(
+                text(
+                    """
+                    CREATE TABLE IF NOT EXISTS chunk_vectors (
+                        chunk_id BIGINT PRIMARY KEY,
+                        svd vector,
+                        updated_at TIMESTAMP
+                    )
+                    """
+                )
+            )
+        except Exception:
+            pass
+    
+    from math import ceil
+    BATCH = 500
+    updated = 0
+    for i in range(0, len(rows), BATCH):
+        batch = rows[i : i + BATCH]
+        payload = []
+        ts = datetime.utcnow()
+        for cid, vec in batch:
+            literal = "[" + ",".join(f"{float(x):.6f}" for x in (vec or [])) + "]"
+            payload.append({"id": int(cid), "svd": literal, "ts": ts})
+        with engine.begin() as conn:
+            try:
+                conn.execute(
+                    text(
+                        """
+                        INSERT INTO chunk_vectors(chunk_id, svd, updated_at)
+                        VALUES(:id, :svd::vector, :ts)
+                        ON CONFLICT (chunk_id) DO UPDATE SET
+                            svd = excluded.svd,
+                            updated_at = excluded.updated_at
+                        """
+                    ),
+                    payload,
+                )
+                updated += len(batch)
+            except Exception:
+                pass
+    return updated
+
+
+def fetch_chunk_vectors(
+    engine: Engine, 
+    ids: List[int]
+) -> Dict[int, List[float]]:
+    """Fetch SVD vectors from pgvector table."""
+    result: Dict[int, List[float]] = {}
+    if not ids:
+        return result
+    if engine.dialect.name != "postgresql":
+        return result
+    
+    safe_ids = []
+    for x in ids:
+        try:
+            safe_ids.append(int(x))
+        except Exception:
+            continue
+    if not safe_ids:
+        return result
+    
+    placeholders = ", ".join(str(i) for i in sorted(set(safe_ids)))
+    with engine.begin() as conn:
+        try:
+            rows = conn.execute(
+                text(f"SELECT chunk_id, svd::text FROM chunk_vectors WHERE chunk_id IN ({placeholders})")
+            )
+            for cid, vec_txt in rows:
+                try:
+                    s = str(vec_txt or "").strip()
+                    if s.startswith("[") and s.endswith("]"):
+                        parts = s[1:-1].split(",")
+                        vec = [float(p) for p in parts if p.strip()]
+                        result[int(cid)] = vec
+                except Exception:
+                    continue
+        except Exception:
+            return result
+    return result
+
+
+# ============================================================================
+# Feedback Operations
+# ============================================================================
+
+def record_feedback(
+    query: str, 
+    chunk_id: int, 
+    score: float,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> None:
+    """Record user feedback."""
+    engine = get_engine(context_db)
+    now = datetime.utcnow().isoformat()
+    with engine.begin() as conn:
+        conn.execute(
+            text("INSERT INTO feedback(query, chunk_id, score, created_at) VALUES(:q, :cid, :s, :now)"),
+            {"q": query.strip(), "cid": chunk_id, "s": score, "now": now}
+        )
+
+
+def fetch_feedback_for_query(
+    query: str,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> List[Tuple[int, float]]:
+    """Return list of (chunk_id, score) for a query."""
+    engine = get_engine(context_db)
+    with engine.begin() as conn:
+        rows = conn.execute(
+            text("SELECT chunk_id, score FROM feedback WHERE query=:q"),
+            {"q": query.strip()}
+        ).fetchall()
+        return [(r[0], r[1]) for r in rows]
+
+
+# ============================================================================
+# Search History
+# ============================================================================
+
+def record_search_history(
+    query: str, 
+    params: dict, 
+    count: int,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> int:
+    """Log a search event."""
+    import json
+    engine = get_engine(context_db)
+    now = datetime.utcnow().isoformat()
+    with engine.begin() as conn:
+        res = conn.execute(
+            text("INSERT INTO search_history(query, params_json, result_count, created_at) VALUES(:q, :p, :c, :now)"),
+            {"q": query, "p": json.dumps(params), "c": count, "now": now}
+        )
+        if res.lastrowid:
+            return int(res.lastrowid)
+        return 0
+
+
+def fetch_search_history(
+    limit: int = 100,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> List[Dict]:
+    """Return recent searches."""
+    import json
+    engine = get_engine(context_db)
+    with engine.begin() as conn:
+        rows = conn.execute(
+            text("SELECT id, query, params_json, result_count, created_at FROM search_history ORDER BY id DESC LIMIT :limit"),
+            {"limit": limit}
+        )
+        out = []
+        for r in rows:
+            try:
+                params = json.loads(r[2])
+            except Exception:
+                params = {}
+            out.append({
+                "id": r[0], "query": r[1], "params": params, "count": r[3], "created_at": r[4]
+            })
+        return out
+
+
+# ============================================================================
+# Factory Reset
+# ============================================================================
+
+def factory_reset_db(context_db: str = DEFAULT_CONTEXT_DB) -> None:
+    """Dangerous: wipe all tables and reinitialize schema."""
+    url = get_database_url(context_db)
+    try:
+        if url.startswith("sqlite///") or url.startswith("sqlite:///"):
+            p = (STATE_DIR / f'context_{context_db}.db')
+            if p.exists():
+                p.unlink()
+            return
+        
+        engine = create_engine(url, future=True, pool_pre_ping=True)
+        with engine.begin() as conn:
+            for tbl in ("apscheduler_jobs", "chunks", "file_index", "jobs", "job_runs", "decay_log"):
+                try:
+                    conn.execute(text(f"DROP TABLE IF EXISTS {tbl}"))
+                except Exception:
+                    pass
+        init_schema(engine)
+    except Exception:
+        return
diff --git a/kb/eval.py b/kb/eval.py
new file mode 100644
index 0000000..d8a83b4
--- /dev/null
+++ b/kb/eval.py
@@ -0,0 +1,156 @@
+import json
+import random
+import numpy as np
+from typing import List, Dict, Tuple
+from sklearn.datasets import fetch_20newsgroups
+from kb.search import HybridSearcher
+from kb.tuning import get_optimizer
+
+def dcg_at_k(r: List[float], k: int) -> float:
+    """Score is discounted cumulative gain at rank k."""
+    r = np.asarray(r)[:k]
+    if r.size:
+        return np.sum(r / np.log2(np.arange(2, r.size + 2)))
+    return 0.0
+
+def ndcg_at_k(r: List[float], k: int) -> float:
+    """Score is normalized discounted cumulative gain (NDCG) at rank k."""
+    dcg_max = dcg_at_k(sorted(r, reverse=True), k)
+    if not dcg_max:
+        return 0.0
+    return dcg_at_k(r, k) / dcg_max
+
+def prepare_training_data(limit: int = 500) -> Tuple[List[Tuple[int, str, str, str]], List[Dict]]:
+    """
+    Load 20 Newsgroups dataset and generate (chunk, query) pairs.
+    Returns: (chunks, queries)
+    """
+    print("Loading 20 Newsgroups dataset (this may download data)...")
+    try:
+        newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
+    except Exception as e:
+        print(f"Failed to load 20newsgroups: {e}")
+        return [], []
+
+    chunks = []
+    queries = []
+    
+    print(f"Processing {min(len(newsgroups.data), limit)} documents...")
+    
+    # Create chunks
+    for i, text in enumerate(newsgroups.data[:limit]):
+        if len(text) < 200:
+            continue
+        
+        cid = i + 100000 # Offset IDs to avoid conflict with local DB
+        path = f"newsgroup_{i}.txt"
+        cname = "content"
+        chunks.append((cid, path, cname, text))
+        
+        # Generate synthetic query: 
+        # Strategy: take a random sentence or phrase from the middle
+        lines = [l.strip() for l in text.splitlines() if len(l.strip()) > 50]
+        if lines:
+            target_line = random.choice(lines)
+            words = target_line.split()
+            if len(words) > 5:
+                # Take a slice of 5-10 words
+                start = random.randint(0, max(0, len(words) - 10))
+                q_text = " ".join(words[start:start+10])
+                queries.append({
+                    "query": q_text,
+                    "relevant_id": cid
+                })
+
+    return chunks, queries
+
+def train_optimizer(n_iter: int = 100):
+    """
+    Run MCTS training loop on the dataset.
+    """
+    chunks, dataset = prepare_training_data(limit=1000)
+    if not chunks:
+        print("No training data available.")
+        return
+
+    print(f"Training on {len(chunks)} documents with {len(dataset)} queries...")
+    
+    # Initialize searcher in memory (no DB persistence needed for training)
+    searcher = HybridSearcher()
+    searcher.fit(chunks)
+    
+    opt = get_optimizer()
+    
+    # Reset optimizer to clean state for training
+    from kb.tuning import SearchOptimizer, ParameterNode
+    opt.root = ParameterNode(SearchOptimizer.DEFAULTS.copy())
+    opt.nodes_map = {opt._param_sig(opt.root.params): opt.root}
+    
+    scores = []
+    
+    for i in range(n_iter):
+        # Pick a random query case
+        case = random.choice(dataset)
+        query = case["query"]
+        target = case["relevant_id"]
+        
+        # 1. Suggest Params (High exploration during training)
+        params = opt.suggest_parameters(exploration_prob=0.4)
+        
+        # 2. Search
+        try:
+            results = searcher.search(
+                query,
+                top_k=params.get("kb_top_k", 5),
+                neighbors=params.get("kb_neighbors", 0),
+                bm25_weight=params.get("kb_bm25_weight", 0.45),
+                lsa_weight=params.get("kb_lsa_weight", 0.2),
+                ann_weight=params.get("kb_ann_weight", 0.15),
+                cand_multiplier=params.get("kb_cand_mult", 5),
+                mmr_diversify=params.get("kb_mmr_diversify", True),
+                mmr_lambda=params.get("kb_mmr_lambda", 0.2),
+                phrase_boost=params.get("kb_phrase_boost", 0.1),
+                min_score=params.get("kb_min_score", 0.005)
+            )
+        except Exception:
+            results = []
+            
+        # 3. Calculate Reward (NDCG@5)
+        # We only have one "relevant" document in this synthetic setup, so relevance is binary
+        relevance_vector = []
+        found = False
+        for cid, score, _, _, _, _ in results:
+            if cid == target:
+                relevance_vector.append(1.0)
+                found = True
+            else:
+                relevance_vector.append(0.0)
+        
+        # Pad if fewer results than k
+        k = 5
+        if len(relevance_vector) < k:
+            relevance_vector.extend([0.0] * (k - len(relevance_vector)))
+            
+        # NDCG calculation
+        # Ideal vector has 1.0 at index 0
+        reward = ndcg_at_k(relevance_vector, k)
+        
+        # Penalize empty results severely
+        if not results:
+            reward = -0.5
+            
+        scores.append(reward)
+        
+        # 4. Update
+        opt.update(params, reward)
+        
+        if i % 10 == 0:
+            avg_score = sum(scores[-20:]) / max(1, len(scores[-20:]))
+            print(f"Iter {i}: Reward={reward:.2f}, Moving Avg={avg_score:.2f}, Params={json.dumps(params)}")
+
+    print("Training complete.")
+    opt.save_model("kb/tuning_model.json")
+    print("Saved model to kb/tuning_model.json")
+
+if __name__ == "__main__":
+    train_optimizer(n_iter=200)
diff --git a/kb/gen_model.py b/kb/gen_model.py
new file mode 100644
index 0000000..b1182fe
--- /dev/null
+++ b/kb/gen_model.py
@@ -0,0 +1,53 @@
+import json
+from kb.tuning import ParameterNode, SearchOptimizer
+
+def generate_pretrained_model():
+    """
+    Construct a synthetic MCTS tree with robust priors.
+    We simulate visits to the default node and some reasonable variations.
+    """
+    defaults = SearchOptimizer.DEFAULTS
+    
+    # Root: Give it solid visits to represent a safe baseline
+    root = ParameterNode(defaults)
+    root.visits = 50
+    root.value = 25.0  # Net positive feedback
+    
+    # Variation 1: Higher recall (top_k=10, cand_mult=8)
+    v1_params = defaults.copy()
+    v1_params["kb_top_k"] = 10
+    v1_params["kb_cand_mult"] = 8
+    v1 = ParameterNode(v1_params, parent=root)
+    v1.visits = 10
+    v1.value = 4.0
+    root.children.append(v1)
+    
+    # Variation 2: Higher precision (bm25=0.8, lsa=0.1)
+    v2_params = defaults.copy()
+    v2_params["kb_bm25_weight"] = 0.8
+    v2_params["kb_lsa_weight"] = 0.1
+    v2 = ParameterNode(v2_params, parent=root)
+    v2.visits = 10
+    v2.value = 6.0 # Slightly better
+    root.children.append(v2)
+    
+    # Variation 3: Semantic heavy (ann=0.4, lsa=0.4, bm25=0.2)
+    v3_params = defaults.copy()
+    v3_params["kb_ann_weight"] = 0.4
+    v3_params["kb_lsa_weight"] = 0.4
+    v3_params["kb_bm25_weight"] = 0.2
+    v3 = ParameterNode(v3_params, parent=root)
+    v3.visits = 10
+    v3.value = 3.0
+    root.children.append(v3)
+    
+    data = root.to_dict()
+    
+    with open("kb/tuning_model.json", "w", encoding="utf-8") as f:
+        json.dump(data, f, indent=2)
+    
+    print("Generated kb/tuning_model.json")
+
+if __name__ == "__main__":
+    generate_pretrained_model()
+
diff --git a/kb/score.py b/kb/score.py
new file mode 100644
index 0000000..0224198
--- /dev/null
+++ b/kb/score.py
@@ -0,0 +1,44 @@
+import math
+from typing import List, Dict, Set
+
+def dcg_at_k(r: List[int], k: int) -> float:
+    """Discounted Cumulative Gain at K."""
+    r = r[:k]
+    if not r:
+        return 0.0
+    return sum(rel / math.log2(idx + 2) for idx, rel in enumerate(r))
+
+def ndcg_at_k(r: List[int], k: int, ground_truth_count: int) -> float:
+    """Normalized DCG at K."""
+    dcg = dcg_at_k(r, k)
+    # Ideal DCG: perfect ordering of all relevant docs
+    # We assume binary relevance (1 or 0) for now, so IDCG is just sum of discounts for first N relevant
+    ideal_rels = [1] * min(k, ground_truth_count)
+    idcg = dcg_at_k(ideal_rels, k)
+    if idcg == 0:
+        return 0.0
+    return dcg / idcg
+
+def precision_at_k(r: List[int], k: int) -> float:
+    """Precision at K."""
+    if k == 0: return 0.0
+    r = r[:k]
+    return sum(r) / k
+
+def recall_at_k(r: List[int], k: int, ground_truth_count: int) -> float:
+    """Recall at K."""
+    if ground_truth_count == 0: return 0.0
+    r = r[:k]
+    return sum(r) / ground_truth_count
+
+def average_precision(r: List[int], ground_truth_count: int) -> float:
+    """Average Precision (AP)."""
+    if ground_truth_count == 0: return 0.0
+    s = 0.0
+    hits = 0
+    for i, rel in enumerate(r):
+        if rel:
+            hits += 1
+            s += hits / (i + 1)
+    return s / ground_truth_count
+
diff --git a/kb/search.py b/kb/search.py
index 6ce625d..6f011cc 100644
--- a/kb/search.py
+++ b/kb/search.py
@@ -9,7 +9,7 @@ from sklearn.feature_extraction.text import CountVectorizer
 from sklearn.cluster import AgglomerativeClustering
 from sklearn.metrics.pairwise import linear_kernel
 from sklearn.decomposition import TruncatedSVD
-from kb.db import get_engine, persist_chunk_vectors, fetch_all_chunks, fetch_chunk_vectors
+from kb.db import get_engine, persist_chunk_vectors, fetch_all_chunks, fetch_chunk_vectors, fetch_feedback_for_query
 
 try:
     from pynndescent import NNDescent  # type: ignore
@@ -346,6 +346,29 @@ class HybridSearcher:
 			final_scores = final_scores + (w_lsa * lsa_norm)
 		if ann_norm is not None:
 			final_scores = final_scores + (w_ann * ann_norm)
+
+		# Bandit/RL: Incorporate explicit user feedback (Rocchio-style adjustment)
+		# We use explicit feedback (thumbs up/down) to adjust scores.
+		# Avoid implicit penalties for re-searching to prevent "negative score" loop.
+		try:
+			feedback = fetch_feedback_for_query(query)
+			if feedback:
+				# Map chunk_id -> aggregate score
+				# sum(scores) so multiple thumbs up boost more, mixed cancel out
+				f_map = {}
+				for cid, sc in feedback:
+					f_map[cid] = f_map.get(cid, 0.0) + sc
+				
+				# Apply boost/penalty
+				# We add (0.2 * aggregate_feedback) to the normalized score
+				# This allows user preference to bubble up items
+				for i, cid in enumerate(self.ids):
+					if cid in f_map:
+						boost = f_map[cid] * 0.2
+						final_scores[i] = np.clip(final_scores[i] + boost, 0.0, 1.0 + abs(boost))
+		except Exception:
+			pass
+
 		# If scores are uniformly low, short-circuit using effective min score
 		try:
 			if final_scores is None or final_scores.size == 0 or float(np.max(final_scores)) < float(ms):
diff --git a/kb/search_enhanced.py b/kb/search_enhanced.py
new file mode 100644
index 0000000..ba08dbc
--- /dev/null
+++ b/kb/search_enhanced.py
@@ -0,0 +1,356 @@
+"""
+Enhanced Search Module for Resume Context Builder
+
+Features:
+- Time range filtering (1 day to 12 months)
+- Multiple context database support
+- Integration with information decay
+"""
+
+from __future__ import annotations
+
+from typing import List, Tuple, Dict, Optional
+import numpy as np
+import re
+
+from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
+from sklearn.cluster import AgglomerativeClustering
+from sklearn.metrics.pairwise import linear_kernel
+from sklearn.decomposition import TruncatedSVD
+
+# Import from enhanced database module
+from kb.db_enhanced import (
+    get_engine,
+    fetch_all_chunks,
+    fetch_chunk_vectors,
+    fetch_feedback_for_query,
+    persist_chunk_vectors,
+    get_time_range,
+    DEFAULT_CONTEXT_DB,
+)
+
+try:
+    from pynndescent import NNDescent
+except Exception:
+    NNDescent = None
+
+
+class HybridSearcher:
+    """Enhanced hybrid searcher with time range support."""
+    
+    def __init__(self, context_db: str = DEFAULT_CONTEXT_DB):
+        self.context_db = context_db
+        self.vectorizer = TfidfVectorizer(
+            strip_accents="unicode",
+            lowercase=True,
+            ngram_range=(1, 2),
+            max_features=50000,
+            stop_words="english",
+            sublinear_tf=True,
+            max_df=0.95,
+            norm="l2",
+        )
+        self.matrix = None
+        self.count_vectorizer = CountVectorizer(
+            strip_accents="unicode",
+            lowercase=True,
+            ngram_range=(1, 2),
+            max_features=50000,
+            stop_words="english",
+            max_df=0.95,
+        )
+        self.count_matrix = None
+        self.doc_lengths: np.ndarray | None = None
+        self.avg_doc_length: float = 0.0
+        self.bm25_idf: np.ndarray | None = None
+        self.bm25_k1: float = 1.2
+        self.bm25_b: float = 0.75
+        self.ids: List[int] = []
+        self.texts: List[str] = []
+        self.meta: List[Tuple[str, str]] = []
+        self.svd: TruncatedSVD | None = None
+        self.doc_vectors_reduced: np.ndarray | None = None
+        self.ann_index = None
+        self.path_to_ordered_indices: Dict[str, List[int]] = {}
+        self.max_pgvector_cache: int = 10000
+        self.enable_ann: bool = True
+        self.svd_components_cap: int = 256
+
+    def fit(
+        self, 
+        docs: List[Tuple[int, str, str, str]],
+        time_range: str | None = None
+    ):
+        """Fit the searcher on documents, optionally filtered by time range.
+        
+        Args:
+            docs: List of (id, path, chunk_name, content) tuples
+            time_range: Optional time range filter (e.g., "1d", "1w", "3m", "1y")
+        """
+        # Apply time range filter if specified
+        if time_range:
+            since, until = get_time_range(time_range)
+            if since or until:
+                # Filter docs by time
+                engine = get_engine(self.context_db)
+                # Get all chunks with their timestamps
+                all_chunks = fetch_all_chunks(engine, since=since, until=until)
+                allowed_ids = {c[0] for c in all_chunks}
+                docs = [d for d in docs if d[0] in allowed_ids]
+        
+        self.ids = [d[0] for d in docs]
+        self.meta = [(d[1], d[2]) for d in docs]
+        self.texts = [d[3] for d in docs]
+        
+        if self.texts:
+            self.matrix = self.vectorizer.fit_transform(self.texts)
+            
+            # Build BM25 structures
+            try:
+                self.count_matrix = self.count_vectorizer.fit_transform(self.texts)
+                n_docs = self.count_matrix.shape[0]
+                df = (self.count_matrix > 0).astype(np.int32).sum(axis=0)
+                df = np.asarray(df).ravel()
+                self.bm25_idf = np.log((n_docs - df + 0.5) / (df + 0.5) + 1.0)
+                self.doc_lengths = np.asarray(self.count_matrix.sum(axis=1)).ravel()
+                self.avg_doc_length = float(self.doc_lengths.mean() if self.doc_lengths.size else 0.0)
+            except Exception:
+                self.count_matrix = None
+                self.doc_lengths = None
+                self.avg_doc_length = 0.0
+                self.bm25_idf = None
+            
+            # Build path -> ordered matrix index list
+            order_map: Dict[str, List[Tuple[int, int]]] = {}
+            for m_idx, (path, cname) in enumerate(self.meta):
+                m = re.search(r"(\d+)$", cname)
+                chunk_num = int(m.group(1)) if m else (m_idx + 1)
+                order_map.setdefault(path, []).append((chunk_num, m_idx))
+            self.path_to_ordered_indices = {
+                p: [mi for _, mi in sorted(pairs, key=lambda x: x[0])] 
+                for p, pairs in order_map.items()
+            }
+            
+            # Try to hydrate SVD vectors from pgvector
+            engine = get_engine(self.context_db)
+            pgvec: Dict[int, List[float]] = {}
+            try:
+                if engine.dialect.name == "postgresql" and len(self.ids) <= self.max_pgvector_cache:
+                    pg = fetch_chunk_vectors(engine, self.ids)
+                    if pg:
+                        pgvec = {cid: vec for cid, vec in pg.items() if vec}
+            except Exception:
+                pgvec = {}
+            
+            # Build ANN index
+            if self.enable_ann and NNDescent is not None and self.matrix.shape[0] >= 10:
+                n_comp = min(
+                    self.svd_components_cap,
+                    max(16, min(self.matrix.shape[0] - 1, self.matrix.shape[1] - 1))
+                )
+                try:
+                    self.svd = TruncatedSVD(n_components=n_comp, random_state=42)
+                    if len(pgvec) == len(self.ids):
+                        try:
+                            doc_vecs = np.array([pgvec.get(cid, []) for cid in self.ids], dtype=np.float32)
+                            if doc_vecs.ndim != 2 or doc_vecs.shape[1] != n_comp:
+                                raise ValueError("pgvector dim mismatch")
+                        except Exception:
+                            doc_vecs = self.svd.fit_transform(self.matrix).astype(np.float32, copy=False)
+                    else:
+                        doc_vecs = self.svd.fit_transform(self.matrix).astype(np.float32, copy=False)
+                    
+                    norms = np.linalg.norm(doc_vecs, axis=1, keepdims=True)
+                    norms[norms == 0] = 1.0
+                    doc_vecs = doc_vecs / norms
+                    self.doc_vectors_reduced = doc_vecs
+                    
+                    try:
+                        n_nbrs = int(min(64, max(10, doc_vecs.shape[0] - 1)))
+                        self.ann_index = NNDescent(doc_vecs, metric="cosine", n_neighbors=n_nbrs, random_state=42)
+                    except Exception:
+                        self.ann_index = None
+                except Exception:
+                    self.svd = None
+                    self.doc_vectors_reduced = None
+                    self.ann_index = None
+            
+            # Persist vectors to pgvector
+            try:
+                if engine.dialect.name == "postgresql" and self.doc_vectors_reduced is not None:
+                    rows = []
+                    for i, cid in enumerate(self.ids):
+                        vec = self.doc_vectors_reduced[i].astype(np.float32).tolist()
+                        rows.append((int(cid), vec))
+                    persist_chunk_vectors(engine, rows)
+            except Exception:
+                pass
+
+    def search(
+        self,
+        query: str,
+        top_k: int = 5,
+        time_range: str | None = None,
+        **kwargs
+    ) -> List[Tuple[int, float, str, str, str, str]]:
+        """Search with optional time range filtering.
+        
+        Args:
+            query: Search query
+            top_k: Number of results to return
+            time_range: Optional time range ("1d", "1w", "1m", "3m", "6m", "1y", "all")
+            **kwargs: Additional search parameters
+        
+        Returns:
+            List of (id, score, path, chunk_name, snippet, full_text) tuples
+        """
+        # If time range specified, we need to filter the corpus
+        if time_range and time_range != "all":
+            engine = get_engine(self.context_db)
+            since, until = get_time_range(time_range)
+            filtered_docs = fetch_all_chunks(engine, since=since, until=until)
+            allowed_ids = {d[0] for d in filtered_docs}
+            
+            # Filter internal data structures
+            filtered_indices = [i for i, cid in enumerate(self.ids) if cid in allowed_ids]
+            if not filtered_indices:
+                return []
+            
+            # Create temporary filtered searcher
+            filtered_ids = [self.ids[i] for i in filtered_indices]
+            filtered_texts = [self.texts[i] for i in filtered_indices]
+            filtered_meta = [self.meta[i] for i in filtered_indices]
+            
+            # Run search on filtered subset
+            return self._search_subset(
+                query, top_k, filtered_ids, filtered_texts, filtered_meta, **kwargs
+            )
+        
+        # No time filtering - search full corpus
+        return self._search_subset(
+            query, top_k, self.ids, self.texts, self.meta, **kwargs
+        )
+
+    def _search_subset(
+        self,
+        query: str,
+        top_k: int,
+        ids: List[int],
+        texts: List[str],
+        meta: List[Tuple[str, str]],
+        **kwargs
+    ) -> List[Tuple[int, float, str, str, str, str]]:
+        """Internal search implementation on a subset of documents."""
+        if not texts:
+            return []
+        
+        query = (query or "").strip()
+        if not query:
+            return []
+        
+        # Transform query
+        q_vec = self.vectorizer.transform([query])
+        k = min(max(1, top_k), len(ids))
+        if k <= 0:
+            return []
+        
+        # Build temporary matrix for subset
+        subset_matrix = self.matrix[[self.ids.index(cid) for cid in ids]]
+        
+        # Compute similarities
+        similarities = linear_kernel(q_vec, subset_matrix).ravel()
+        
+        # Get top-k indices
+        top_indices = np.argsort(similarities)[::-1][:k]
+        
+        results = []
+        for idx in top_indices:
+            cid = ids[idx]
+            score = float(similarities[idx])
+            path, cname = meta[idx]
+            text = texts[idx]
+            
+            # Build snippet
+            snippet = text[:400] + "..." if len(text) > 400 else text
+            
+            results.append((cid, score, path, cname, snippet, text))
+        
+        return results
+
+    def fit_from_db(
+        self,
+        time_range: str | None = None,
+        limit: int | None = None
+    ):
+        """Fit searcher directly from database with optional time filtering.
+        
+        Args:
+            time_range: Optional time range filter
+            limit: Maximum number of chunks to load
+        """
+        engine = get_engine(self.context_db)
+        
+        since, until = (None, None)
+        if time_range and time_range != "all":
+            since, until = get_time_range(time_range)
+        
+        docs = fetch_all_chunks(engine, since=since, until=until)
+        
+        if limit and len(docs) > limit:
+            docs = docs[:limit]
+        
+        self.fit(docs)
+
+
+def search_with_time_filter(
+    query: str,
+    time_range: str = "all",
+    top_k: int = 5,
+    context_db: str = DEFAULT_CONTEXT_DB
+) -> List[Tuple[int, float, str, str, str, str]]:
+    """Convenience function for searching with time filter.
+    
+    Args:
+        query: Search query
+        time_range: Time range ("1d", "1w", "1m", "3m", "6m", "1y", "all")
+        top_k: Number of results
+        context_db: Context database name
+    
+    Returns:
+        Search results
+    """
+    searcher = HybridSearcher(context_db)
+    searcher.fit_from_db(time_range=time_range)
+    return searcher.search(query, top_k=top_k, time_range=time_range)
+
+
+if __name__ == "__main__":
+    # CLI interface
+    import argparse
+    
+    parser = argparse.ArgumentParser(description="Enhanced search for Resume Context Builder")
+    parser.add_argument("query", help="Search query")
+    parser.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB,
+                        help="Context database name")
+    parser.add_argument("--time-range", "-t", default="all",
+                        choices=["1d", "1w", "1m", "3m", "6m", "1y", "all"],
+                        help="Time range filter")
+    parser.add_argument("--top-k", "-k", type=int, default=5,
+                        help="Number of results")
+    
+    args = parser.parse_args()
+    
+    results = search_with_time_filter(
+        args.query,
+        time_range=args.time_range,
+        top_k=args.top_k,
+        context_db=args.context
+    )
+    
+    print(f"Search results for '{args.query}' (time range: {args.time_range}):")
+    print("-" * 60)
+    for cid, score, path, cname, snippet, _ in results:
+        print(f"\nScore: {score:.4f} | Chunk: {cname}")
+        print(f"Path: {path}")
+        print(f"Snippet: {snippet[:200]}...")
+        print("-" * 60)
diff --git a/kb/train_model.py b/kb/train_model.py
new file mode 100644
index 0000000..de34b7b
--- /dev/null
+++ b/kb/train_model.py
@@ -0,0 +1,171 @@
+import os
+import json
+import zipfile
+import tempfile
+import random
+import urllib.request
+from pathlib import Path
+from typing import List, Dict, Tuple
+
+from kb.search import HybridSearcher
+from kb.tuning import get_optimizer
+from kb.score import ndcg_at_k
+
+DATASET_URL = "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip"
+
+def download_and_load_scifact() -> Tuple[List[Tuple[int, str, str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:
+    """
+    Downloads SciFact dataset.
+    Returns:
+        corpus: List[(id_int, path, title, text)]
+        queries: Dict[query_id_str, text]
+        qrels: Dict[query_id_str, Dict[doc_id_str, score]]
+    """
+    print(f"Downloading dataset from {DATASET_URL}...")
+    with tempfile.TemporaryDirectory() as tmpdir:
+        zip_path = Path(tmpdir) / "scifact.zip"
+        urllib.request.urlretrieve(DATASET_URL, zip_path)
+        
+        with zipfile.ZipFile(zip_path, 'r') as zf:
+            zf.extractall(tmpdir)
+        
+        base = Path(tmpdir) / "scifact"
+        
+        # Load Corpus
+        corpus = []
+        print("Loading corpus...")
+        with open(base / "corpus.jsonl", "r", encoding="utf-8") as f:
+            for line in f:
+                doc = json.loads(line)
+                doc_id = doc.get("_id")
+                title = doc.get("title", "")
+                text = doc.get("text", "")
+                full_text = f"{title}\n\n{text}"
+                # We map string ID to int for HybridSearcher compatibility
+                try:
+                    cid = int(doc_id)
+                    corpus.append((cid, f"scifact_{cid}", title, full_text))
+                except ValueError:
+                    pass
+        
+        # Load Queries
+        queries = {}
+        print("Loading queries...")
+        with open(base / "queries.jsonl", "r", encoding="utf-8") as f:
+            for line in f:
+                q = json.loads(line)
+                queries[q.get("_id")] = q.get("text")
+                
+        # Load Qrels
+        qrels = {}
+        print("Loading qrels...")
+        # Check both test and dev
+        for split in ["test.tsv", "dev.tsv"]:
+            p = base / "qrels" / split
+            if p.exists():
+                with open(p, "r", encoding="utf-8") as f:
+                    next(f) # header
+                    for line in f:
+                        parts = line.strip().split("\t")
+                        if len(parts) >= 3:
+                            qid, docid, score = parts[0], parts[1], int(parts[2])
+                            if score > 0:
+                                if qid not in qrels: qrels[qid] = {}
+                                qrels[qid][docid] = score
+                                
+        return corpus, queries, qrels
+
+def train_optimizer():
+    corpus, queries, qrels = download_and_load_scifact()
+    print(f"Loaded {len(corpus)} docs, {len(queries)} queries, {len(qrels)} qrels.")
+    
+    # 1. Fit Searcher
+    print("Indexing corpus...")
+    searcher = HybridSearcher()
+    searcher.fit(corpus)
+    
+    # 2. Training Loop
+    print("Starting MCTS training loop...")
+    optimizer = get_optimizer()
+    
+    # Filter queries to those with qrels
+    valid_qids = [k for k in queries.keys() if k in qrels]
+    random.shuffle(valid_qids)
+    
+    # Run for N iterations (enough to converge parameters slightly)
+    # We loop through queries multiple times if needed, but 1 pass over ~300 queries is decent
+    # SciFact has small qrels, so we iterate carefully
+    
+    metrics = []
+    
+    # Exploration phase (high epsilon) -> Exploitation (low epsilon)
+    epochs = 3
+    for epoch in range(epochs):
+        print(f"Epoch {epoch+1}/{epochs}")
+        random.shuffle(valid_qids)
+        
+        for i, qid in enumerate(valid_qids):
+            query_text = queries[qid]
+            rel_map = qrels[qid]
+            
+            # Select params
+            # Anneal exploration: 0.5 -> 0.1
+            progress = (epoch * len(valid_qids) + i) / (epochs * len(valid_qids))
+            epsilon = 0.5 * (1 - progress) + 0.1
+            
+            params = optimizer.suggest_parameters(exploration_prob=epsilon)
+            
+            # Search
+            results = searcher.search(
+                query_text,
+                top_k=max(10, params.get("kb_top_k", 10)), # Force higher k for eval visibility
+                neighbors=params.get("kb_neighbors", 0),
+                bm25_weight=params.get("kb_bm25_weight", 0.45),
+                lsa_weight=params.get("kb_lsa_weight", 0.2),
+                ann_weight=params.get("kb_ann_weight", 0.15),
+                cand_multiplier=params.get("kb_cand_mult", 5),
+                mmr_diversify=params.get("kb_mmr_diversify", True),
+                mmr_lambda=params.get("kb_mmr_lambda", 0.2),
+                phrase_boost=params.get("kb_phrase_boost", 0.1),
+                min_score=0.0 # Disable threshold for eval to get ranking
+            )
+            
+            # Convert results to binary relevance vector
+            # rel_map keys are strings, result cids are ints
+            retrieved_rels = []
+            for cid, score, _, _, _, _ in results:
+                is_rel = 1 if str(cid) in rel_map else 0
+                retrieved_rels.append(is_rel)
+            
+            # Calculate Reward (NDCG@10)
+            ground_truth_count = len(rel_map)
+            score = ndcg_at_k(retrieved_rels, 10, ground_truth_count)
+            
+            # Update Optimizer
+            # Map NDCG 0..1 to something slightly more punitive for 0
+            # If score is high -> reward +1.0
+            # If score is 0 -> reward -0.5
+            reward = (score * 2.0) - 0.5
+            
+            optimizer.update(params, reward)
+            
+            metrics.append(score)
+            if i % 20 == 0:
+                avg = sum(metrics[-50:]) / max(1, len(metrics[-50:]))
+                print(f"Step {i}: Avg NDCG@10: {avg:.3f} | Params: {json.dumps(params)}")
+
+    # 3. Save
+    out_path = Path("kb/tuning_model.json")
+    print(f"Saving model to {out_path.absolute()}")
+    optimizer.save_model(str(out_path.absolute()))
+    
+    # Print best path
+    print("Root visits:", optimizer.root.visits)
+    if optimizer.root.children:
+        best = max(optimizer.root.children, key=lambda c: c.value / max(1, c.visits))
+        print("Best Params found:", best.params)
+        print("Best Value:", best.value / max(1, best.visits))
+
+if __name__ == "__main__":
+    train_optimizer()
+
diff --git a/kb/tuning.py b/kb/tuning.py
new file mode 100644
index 0000000..60d15a0
--- /dev/null
+++ b/kb/tuning.py
@@ -0,0 +1,231 @@
+import math
+import random
+import json
+import importlib.resources as resources
+from typing import Dict, List, Any, Optional
+from kb.db import fetch_search_history, fetch_feedback_for_query
+
+class ParameterNode:
+    def __init__(self, params: Dict[str, Any], parent=None):
+        self.params = params
+        self.parent = parent
+        self.children: List['ParameterNode'] = []
+        self.visits = 0
+        self.value = 0.0  # Cumulative reward
+
+    def uct_score(self, total_visits: int, exploration_weight: float = 1.41) -> float:
+        if self.visits == 0:
+            return float('inf')
+        return (self.value / self.visits) + exploration_weight * math.sqrt(math.log(total_visits) / self.visits)
+
+    def to_dict(self):
+        """Serialize for storage."""
+        return {
+            "params": self.params,
+            "visits": self.visits,
+            "value": self.value,
+            "children": [c.to_dict() for c in self.children]
+        }
+
+    @classmethod
+    def from_dict(cls, data: Dict, parent=None) -> 'ParameterNode':
+        node = cls(data["params"], parent)
+        node.visits = data["visits"]
+        node.value = data["value"]
+        node.children = [cls.from_dict(c, parent=node) for c in data.get("children", [])]
+        return node
+
+class SearchOptimizer:
+    """
+    Monte Carlo Tree Search (MCTS) inspired optimizer for search parameters.
+    Treats parameter selection as a tree search where:
+    - Nodes are parameter configurations.
+    - Expansion adds slight perturbations to parameters.
+    - Selection uses UCT.
+    - Simulation is replaced by actual user feedback (online learning).
+    """
+
+    # Default baseline parameters
+    DEFAULTS = {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": True,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1,
+    }
+
+    # parameter bounds and step sizes for mutation
+    RANGES = {
+        "kb_top_k": (1, 20, int),
+        "kb_min_score": (0.0, 0.5, float),
+        "kb_neighbors": (0, 5, int),
+        "kb_bm25_weight": (0.0, 1.0, float),
+        "kb_lsa_weight": (0.0, 1.0, float),
+        "kb_ann_weight": (0.0, 1.0, float),
+        "kb_cand_mult": (1, 10, int),
+        "kb_mmr_lambda": (0.0, 1.0, float),
+        "kb_phrase_boost": (0.0, 1.0, float),
+    }
+
+    def __init__(self):
+        self.root = ParameterNode(self.DEFAULTS.copy())
+        self.nodes_map = {} # Signature -> Node to reuse states
+        self.nodes_map[self._param_sig(self.root.params)] = self.root
+        
+        # 1. Try to load pretrained model from package
+        self._load_pretrained()
+        
+        # 2. Hydrate/Update from local history (user specific tuning)
+        self._hydrate_from_history()
+
+    def _param_sig(self, params: Dict) -> str:
+        # Canonical string for params
+        return json.dumps(params, sort_keys=True)
+
+    def _load_pretrained(self):
+        """Load static pretrained model if available."""
+        try:
+            # Look for tuning_model.json in kb package
+            ref = resources.files("kb").joinpath("tuning_model.json")
+            if ref and ref.is_file():
+                data = json.loads(ref.read_text(encoding="utf-8"))
+                # Replace root with loaded tree
+                self.root = ParameterNode.from_dict(data)
+                # Rebuild map
+                self.nodes_map = {}
+                self._rebuild_map(self.root)
+        except Exception:
+            pass
+
+    def _rebuild_map(self, node: ParameterNode):
+        sig = self._param_sig(node.params)
+        self.nodes_map[sig] = node
+        for c in node.children:
+            self._rebuild_map(c)
+
+    def _hydrate_from_history(self):
+        """Rebuild limited tree stats from DB history."""
+        try:
+            history = fetch_search_history(limit=500)
+            for h in history:
+                q = h["query"]
+                params = h["params"]
+                # Try to get feedback
+                feedback = fetch_feedback_for_query(q)
+                if not feedback:
+                    continue
+                
+                # Simple reward: +1 for each positive, -1 for negative
+                reward = sum(score for _, score in feedback)
+                
+                # Find or create node
+                sig = self._param_sig(params)
+                if sig not in self.nodes_map:
+                    node = ParameterNode(params, parent=self.root) # simplify: flat attach to root for hydration
+                    self.root.children.append(node)
+                    self.nodes_map[sig] = node
+                else:
+                    node = self.nodes_map[sig]
+                
+                node.visits += 1
+                node.value += reward
+                self.root.visits += 1
+        except Exception:
+            pass
+
+    def save_model(self, path: str):
+        """Save current tree state to JSON."""
+        data = self.root.to_dict()
+        with open(path, "w", encoding="utf-8") as f:
+            json.dump(data, f, indent=2)
+
+    def suggest_parameters(self, exploration_prob: float = 0.2) -> Dict[str, Any]:
+        """Select parameters for the next search query."""
+        
+        # Epsilon-greedy: sometimes purely explore
+        if random.random() < exploration_prob:
+             return self._mutate(self.root.params)
+
+        # Selection: Traverse best UCT path
+        node = self.root
+        # Depth limit 
+        for _ in range(3):
+            if not node.children:
+                # If leaf, expand
+                if node.visits > 0:
+                    self._expand(node)
+                    if node.children:
+                        node = random.choice(node.children)
+                break
+            
+            # Choose best child
+            best_child = max(node.children, key=lambda c: c.uct_score(node.visits))
+            node = best_child
+        
+        return node.params
+
+    def _expand(self, node: ParameterNode, num_children=3):
+        """Create mutated children."""
+        for _ in range(num_children):
+            new_params = self._mutate(node.params)
+            sig = self._param_sig(new_params)
+            if sig not in self.nodes_map:
+                child = ParameterNode(new_params, parent=node)
+                node.children.append(child)
+                self.nodes_map[sig] = child
+
+    def _mutate(self, params: Dict[str, Any]) -> Dict[str, Any]:
+        """Perturb one random parameter."""
+        new_p = params.copy()
+        keys = list(self.RANGES.keys())
+        k = random.choice(keys)
+        min_v, max_v, dtype = self.RANGES[k]
+        
+        curr = new_p.get(k, self.DEFAULTS.get(k))
+        
+        if dtype == int:
+            step = 1 if random.random() < 0.5 else -1
+            curr += step
+        else:
+            step = 0.05 if random.random() < 0.5 else -0.05
+            curr += step
+        
+        # Clamp
+        if dtype == int:
+            curr = max(min_v, min(max_v, int(round(curr))))
+        else:
+            curr = max(min_v, min(max_v, float(curr)))
+            
+        new_p[k] = curr
+        return new_p
+
+    def update(self, params: Dict[str, Any], reward: float):
+        """Update stats for a parameter set (Backpropagation)."""
+        sig = self._param_sig(params)
+        node = self.nodes_map.get(sig)
+        if not node:
+            # If we used params not in tree (e.g. random explore), attach to root
+            node = ParameterNode(params, parent=self.root)
+            self.root.children.append(node)
+            self.nodes_map[sig] = node
+        
+        # Backprop
+        curr = node
+        while curr:
+            curr.visits += 1
+            curr.value += reward
+            curr = curr.parent
+
+# Singleton instance
+_optimizer = None
+def get_optimizer():
+    global _optimizer
+    if _optimizer is None:
+        _optimizer = SearchOptimizer()
+    return _optimizer
+
diff --git a/kb/tuning_model.json b/kb/tuning_model.json
new file mode 100644
index 0000000..c7195e5
--- /dev/null
+++ b/kb/tuning_model.json
@@ -0,0 +1,2797 @@
+{
+  "params": {
+    "kb_top_k": 5,
+    "kb_min_score": 0.005,
+    "kb_neighbors": 0,
+    "kb_bm25_weight": 0.45,
+    "kb_lsa_weight": 0.2,
+    "kb_ann_weight": 0.15,
+    "kb_cand_mult": 5,
+    "kb_mmr_diversify": true,
+    "kb_mmr_lambda": 0.2,
+    "kb_phrase_boost": 0.1
+  },
+  "visits": 1100,
+  "value": 702.2645631584643,
+  "children": [
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.05
+      },
+      "visits": 85,
+      "value": 58.719315000045334,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 6,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.05
+          },
+          "visits": 11,
+          "value": 4.0,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 5,
+              "value": 1.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 5,
+              "value": 2.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.25,
+            "kb_phrase_boost": 0.05
+          },
+          "visits": 26,
+          "value": 18.10933261197483,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 25,
+              "value": 17.10933261197483,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 1,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.05
+          },
+          "visits": 26,
+          "value": 17.22258199935364,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 13,
+              "value": 9.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 12,
+              "value": 7.222581999353641,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.0,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 66,
+      "value": 42.524312750126114,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.0,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.15000000000000002
+          },
+          "visits": 13,
+          "value": 5.5,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.05,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 7,
+              "value": 3.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 5,
+              "value": 1.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.0,
+            "kb_neighbors": 1,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 23,
+          "value": 13.522459119332726,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 10,
+              "value": 7.309270736081082,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 3,
+              "value": 0.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 9,
+              "value": 5.213188383251645,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 1,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 63,
+      "value": 40.0886732251608,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 1,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.09999999999999999,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 14,
+          "value": 7.104334606358991,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 2.438557452045513,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.04999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 2.2262943855309167,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 1.439482768782562,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.055,
+            "kb_neighbors": 1,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 21,
+          "value": 14.646931637575534,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 2.261859507142915,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 6,
+              "value": 3.885072130432616,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 9,
+              "value": 7.5,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 6,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 1,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 16,
+          "value": 9.063701366757181,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 7,
+              "value": 5.2618595071429155,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 3,
+              "value": 0.7737056144690833,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 2.028136245145182,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.09999999999999999,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 78,
+      "value": 53.603819231909355,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 6,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.09999999999999999,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 24,
+          "value": 12.974122573657512,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 23,
+              "value": 11.974122573657512,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.15000000000000002,
+            "kb_ann_weight": 0.09999999999999999,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 32,
+          "value": 19.59413153663985,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 15,
+              "value": 9.078129652635775,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 16,
+              "value": 9.516001884004073,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.15000000000000002,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 81,
+      "value": 56.57028193862438,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.10000000000000002,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 25,
+          "value": 16.96341310747475,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.10000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 2,
+              "value": -0.3979400086720376,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.10000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 0.3613531161467861,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.10000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 18,
+              "value": 16.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.4,
+            "kb_lsa_weight": 0.15000000000000002,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 27,
+          "value": 19.137413788533603,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 10,
+              "value": 8.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 12,
+              "value": 9.863708174064522,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.10000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 0.27370561446908326,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.4,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 60,
+      "value": 37.82508737755057,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.4,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.15000000000000002,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 22,
+          "value": 12.309270736081082,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 6,
+              "value": 2.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 7,
+              "value": 4.273705614469083,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.10000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 8,
+              "value": 5.035565121611999,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 1,
+            "kb_bm25_weight": 0.4,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 22,
+          "value": 13.725272614617664,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 1.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 14,
+              "value": 11.725272614617664,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 3,
+              "value": 0.0,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.25,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 95,
+      "value": 68.94746350089734,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.25,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 4,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 36,
+          "value": 27.15934261459313,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 0.9579380788056291,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 17,
+              "value": 14.272878361977916,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 14,
+              "value": 10.928526173809582,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 4,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.25,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 30,
+          "value": 21.475092472674085,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 0.21241437421604437,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 25,
+              "value": 20.26267809845804,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.25,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.25,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 9,
+          "value": 2.3664911593324383,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 1.3664911593324383,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.3,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 3,
+              "value": 0.0,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.15000000000000002,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 65,
+      "value": 41.73268855492161,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 4,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.15000000000000002,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 30,
+          "value": 22.617049742256587,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 13,
+              "value": 8.617049742256587,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 16,
+              "value": 13.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.15000000000000002,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.15000000000000002,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 15,
+          "value": 7.751719314194142,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 8,
+              "value": 3.97801369972506,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 6,
+              "value": 2.7737056144690833,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.25,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 71,
+      "value": 46.64406961497896,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.25,
+            "kb_phrase_boost": 0.15000000000000002
+          },
+          "visits": 19,
+          "value": 9.41722764796573,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.3,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 4,
+              "value": 1.4403722811357498,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 2,
+              "value": -1.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 12,
+              "value": 8.97685536682998,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 4,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.25,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 27,
+          "value": 14.964982459870315,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 3,
+              "value": -0.9218703473642242,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 23,
+              "value": 14.88685280723454,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 4,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 59,
+      "value": 36.39102138444055,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.4,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 4,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 20,
+          "value": 12.392789260714373,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 6,
+              "value": 4.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 7,
+              "value": 4.892789260714373,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 6,
+              "value": 2.5,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.0,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 4,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 17,
+          "value": 9.102141138458556,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 11,
+              "value": 6.630929753571458,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 1.471211384887099,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 4,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 67,
+      "value": 42.747829398398636,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 4,
+            "kb_min_score": 0.0,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 21,
+          "value": 14.087647501677703,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 9,
+              "value": 7.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 6,
+              "value": 3.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 3.087647501677703,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 4,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.25,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 18,
+          "value": 11.91375665014152,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 2.261859507142915,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 3,
+              "value": 1.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 9,
+              "value": 7.651897142998606,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.15000000000000002
+      },
+      "visits": 35,
+      "value": 16.12473872670236,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 4,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.15000000000000002
+          },
+          "visits": 6,
+          "value": 3.1309297535714578,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 2,
+              "value": 0.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 3,
+              "value": 1.6309297535714578,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.4,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.15000000000000002
+          },
+          "visits": 8,
+          "value": 5.0,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.10000000000000002
+              },
+              "visits": 3,
+              "value": 1.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 4,
+              "value": 2.5,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 6,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 72,
+      "value": 48.0869822449517,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.25,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 6,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 18,
+          "value": 11.59748650464866,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 8,
+              "value": 6.7618595071429155,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 6,
+              "value": 3.1232126232897013,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 3,
+              "value": 0.7124143742160444,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 6,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.15000000000000002,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 9,
+          "value": 3.2737056144690833,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 2.2737056144690833,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": -0.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 2,
+              "value": 0.5,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.0,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 6,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 22,
+          "value": 16.061141357976673,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 21,
+              "value": 15.061141357976672,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.055,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 75,
+      "value": 50.72136399546769,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.055,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 6,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 19,
+          "value": 11.063584904425452,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 11,
+              "value": 6.428526173809582,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.10500000000000001,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 7,
+              "value": 3.635058730615869,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.0049999999999999975,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 26,
+          "value": 18.08713970110166,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0049999999999999975,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 10,
+              "value": 5.087139701101659,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0049999999999999975,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 15,
+              "value": 12.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.055,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 4,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 18,
+          "value": 9.709286273793786,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 9,
+              "value": 5.780760099984203,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 3,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 1.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 1.4285261738095816,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 6,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 35,
+      "value": 16.168367864316423,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 6,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.25,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 4,
+          "value": 1.4042146930010952,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": -0.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 6,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": -0.09578530699890486,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.25,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": 1.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 6,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.15000000000000002,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 4,
+          "value": 1.261859507142915,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 1,
+              "value": 0.7618595071429151,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": 1.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 6,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": -0.5,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 7,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 6,
+          "value": 4.5,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 7,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 2,
+              "value": 3.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 7,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 2,
+              "value": 1.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 7,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": -0.5,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.5,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.15,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 44,
+      "value": 23.729156555000674,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 1,
+            "kb_bm25_weight": 0.5,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 7,
+          "value": 4.0,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 4,
+              "value": 2.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.25,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 2,
+              "value": 1.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.5,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.05
+          },
+          "visits": 6,
+          "value": 2.5,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 4,
+              "value": 2.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 1,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 1,
+              "value": -0.5,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.5,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.09999999999999999,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 9,
+          "value": 5.816325221596202,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.0,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.5,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.09999999999999999,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 8,
+              "value": 4.816325221596202,
+              "children": []
+            }
+          ]
+        }
+      ]
+    },
+    {
+      "params": {
+        "kb_top_k": 5,
+        "kb_min_score": 0.005,
+        "kb_neighbors": 0,
+        "kb_bm25_weight": 0.45,
+        "kb_lsa_weight": 0.2,
+        "kb_ann_weight": 0.2,
+        "kb_cand_mult": 5,
+        "kb_mmr_diversify": true,
+        "kb_mmr_lambda": 0.2,
+        "kb_phrase_boost": 0.1
+      },
+      "visits": 32,
+      "value": 13.672835043893626,
+      "children": [
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.15000000000000002,
+            "kb_ann_weight": 0.2,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 2,
+          "value": -1.0,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 4,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 0,
+              "value": 0.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.15000000000000002
+              },
+              "visits": 1,
+              "value": -0.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.15000000000000002,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.05
+              },
+              "visits": 0,
+              "value": 0.0,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.15000000000000002,
+            "kb_cand_mult": 5,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 12,
+          "value": 6.497992895644791,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.4,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15000000000000002,
+                "kb_cand_mult": 5,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 6,
+              "value": 2.261859507142915,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.15000000000000002,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 5,
+              "value": 2.7361333885018744,
+              "children": []
+            }
+          ]
+        },
+        {
+          "params": {
+            "kb_top_k": 5,
+            "kb_min_score": 0.005,
+            "kb_neighbors": 0,
+            "kb_bm25_weight": 0.45,
+            "kb_lsa_weight": 0.2,
+            "kb_ann_weight": 0.2,
+            "kb_cand_mult": 4,
+            "kb_mmr_diversify": true,
+            "kb_mmr_lambda": 0.2,
+            "kb_phrase_boost": 0.1
+          },
+          "visits": 2,
+          "value": -1.0,
+          "children": [
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 3,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 1,
+              "value": -0.5,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.055,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.2,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 0,
+              "value": 0.0,
+              "children": []
+            },
+            {
+              "params": {
+                "kb_top_k": 5,
+                "kb_min_score": 0.005,
+                "kb_neighbors": 0,
+                "kb_bm25_weight": 0.45,
+                "kb_lsa_weight": 0.2,
+                "kb_ann_weight": 0.2,
+                "kb_cand_mult": 4,
+                "kb_mmr_diversify": true,
+                "kb_mmr_lambda": 0.15000000000000002,
+                "kb_phrase_boost": 0.1
+              },
+              "visits": 0,
+              "value": 0.0,
+              "children": []
+            }
+          ]
+        }
+      ]
+    }
+  ]
+}
\ No newline at end of file
diff --git a/kb/upsert_enhanced.py b/kb/upsert_enhanced.py
new file mode 100644
index 0000000..de2e3d8
--- /dev/null
+++ b/kb/upsert_enhanced.py
@@ -0,0 +1,337 @@
+"""
+Enhanced Upsert Module for Resume Context Builder
+
+Features:
+- Multiple context database support
+- Proper file deletion
+- Time-based filtering
+- Information decay integration
+"""
+
+import os
+import sys
+from pathlib import Path
+from typing import List, Tuple, Optional, Callable
+import re
+import hashlib
+from datetime import datetime
+
+from context_packager_data.tokenizer import get_encoding
+
+# Import from enhanced database module
+from kb.db_enhanced import (
+    get_engine,
+    upsert_chunks,
+    get_file_index,
+    upsert_file_index,
+    delete_chunks_by_path,
+    delete_file_index,
+    fetch_chunk_ids_by_path,
+    DEFAULT_CONTEXT_DB,
+    list_context_databases,
+    delete_context_database,
+    apply_information_decay,
+    get_time_range,
+)
+from kb.graph import build_meta_graph
+
+
+def _find_prev_boundary(text: str, approx_char_idx: int, window: int = 500) -> int:
+    """Find a reasonable previous boundary near approx_char_idx."""
+    start = max(0, approx_char_idx - window)
+    snippet = text[start:approx_char_idx]
+    bl = snippet.rfind("\n\n")
+    if bl != -1:
+        return start + bl + 2
+    for pat in (r"\n# ", r"\n## ", r"\n### "):
+        m = list(re.finditer(pat, snippet))
+        if m:
+            return start + m[-1].start() + 1
+    return approx_char_idx
+
+
+def _find_next_boundary(text: str, approx_char_idx: int, window: int = 500) -> int:
+    """Find a reasonable next boundary near approx_char_idx."""
+    end = min(len(text), approx_char_idx + window)
+    snippet = text[approx_char_idx:end]
+    if approx_char_idx <= 0 or approx_char_idx >= len(text):
+        return max(0, min(len(text), approx_char_idx))
+    if text[approx_char_idx - 1].isspace():
+        return approx_char_idx
+    bl = snippet.find("\n\n")
+    if bl != -1:
+        return approx_char_idx + bl + 2
+    for pat in (r"\n# ", r"\n## ", r"\n### "):
+        m = re.search(pat, snippet)
+        if m:
+            return approx_char_idx + m.start() + 1
+    m = re.search(r"\s", snippet)
+    if m:
+        pos = approx_char_idx + m.start()
+        while pos < len(text) and text[pos].isspace():
+            pos += 1
+        return pos
+    return approx_char_idx
+
+
+def slice_text_tokens(
+    text: str,
+    max_tokens: int = 1500,
+    overlap_tokens: int = 150,
+    encoding_name: str = "o200k_base",
+) -> List[str]:
+    """Token-based chunking with overlap."""
+    if not text:
+        return []
+    if max_tokens <= 0:
+        return [text]
+    
+    enc = get_encoding(encoding_name)
+    toks = enc.encode(text)
+    chunks: List[str] = []
+    step = max(1, max_tokens - max(0, overlap_tokens))
+    
+    for i in range(0, len(toks), step):
+        chunk_toks = toks[i : i + max_tokens]
+        if not chunk_toks:
+            break
+        candidate = enc.decode(chunk_toks)
+        approx_start = len(enc.decode(toks[:i]))
+        approx_end = len(enc.decode(toks[: i + max_tokens]))
+        start_adj = 0 if i == 0 else _find_next_boundary(text, approx_start)
+        end_adj = _find_prev_boundary(text, approx_end)
+        
+        if end_adj <= start_adj:
+            start_adj = approx_start
+            end_adj = max(approx_end, start_adj)
+        
+        start_adj = max(0, min(len(text), start_adj))
+        end_adj = max(start_adj, min(len(text), end_adj))
+        refined = text[start_adj:end_adj].strip()
+        chunks.append(refined if refined else candidate)
+    
+    return chunks
+
+
+def upsert_markdown_files(
+    md_files: List[Path],
+    *,
+    max_tokens_per_chunk: Optional[int] = 1500,
+    overlap_tokens: int = 150,
+    encoding_name: str = "o200k_base",
+    progress_cb: Optional[Callable[[int, int, Path], None]] = None,
+    cancel_cb: Optional[Callable[[], bool]] = None,
+    context_db: str = DEFAULT_CONTEXT_DB,
+) -> int:
+    """Upsert markdown files into a specific context database.
+    
+    Args:
+        md_files: List of markdown file paths
+        max_tokens_per_chunk: Maximum tokens per chunk
+        overlap_tokens: Overlap between chunks
+        encoding_name: Tokenizer encoding name
+        progress_cb: Callback for progress updates
+        cancel_cb: Callback to check for cancellation
+        context_db: Context database name (default: "default")
+    
+    Returns:
+        Number of chunks upserted
+    """
+    engine = get_engine(context_db)
+    records: List[Tuple[str, str, str]] = []
+    total = len(md_files)
+    
+    for idx, md in enumerate(md_files):
+        if cancel_cb is not None:
+            try:
+                if cancel_cb():
+                    break
+            except Exception:
+                pass
+        
+        if progress_cb is not None:
+            try:
+                progress_cb(idx + 1, total, md)
+            except Exception:
+                pass
+        
+        try:
+            content = md.read_text(encoding="utf-8")
+            file_sha = hashlib.sha256(content.encode("utf-8")).hexdigest()
+            params_sig = f"t={max_tokens_per_chunk}|o={overlap_tokens}|e={encoding_name}"
+            
+            prev = get_file_index(engine, str(md))
+            if prev and prev[0] == file_sha and prev[1] == params_sig:
+                # Unchanged - skip
+                continue
+            else:
+                # Changed or new - delete old chunks first (proper deletion)
+                deleted_count = delete_chunks_by_path(engine, str(md))
+                if deleted_count > 0:
+                    print(f"[info] Replaced {deleted_count} old chunks for {md.name}")
+            
+            # Get file modification time as source date
+            file_mtime = datetime.fromtimestamp(md.stat().st_mtime).isoformat()
+            
+            if max_tokens_per_chunk and max_tokens_per_chunk > 0:
+                chunks = slice_text_tokens(
+                    content,
+                    max_tokens=max_tokens_per_chunk,
+                    overlap_tokens=overlap_tokens,
+                    encoding_name=encoding_name,
+                )
+            else:
+                chunks = [content]
+            
+            for i, ch in enumerate(chunks):
+                records.append((str(md), f"part{i+1}", ch))
+            
+            upsert_file_index(engine, str(md), file_sha, params_sig, file_date=file_mtime)
+            
+        except Exception as e:
+            print(f"[warn] Failed to process {md}: {e}", file=sys.stderr)
+            continue
+    
+    if records:
+        upsert_chunks(engine, records)
+        
+        # Update Meta-Graph for new chunks
+        paths = list(set(r[0] for r in records))
+        all_new_ids = []
+        for p in paths:
+            all_new_ids.extend(fetch_chunk_ids_by_path(engine, p))
+        
+        if all_new_ids:
+            build_meta_graph(all_new_ids)
+    
+    return len(records)
+
+
+def delete_file_from_context(
+    file_path: str,
+    context_db: str = DEFAULT_CONTEXT_DB,
+) -> int:
+    """Completely delete a file and all its chunks from a context database.
+    
+    This is the fixed deletion function that properly removes:
+    - All chunks for the file
+    - The file index entry
+    
+    Args:
+        file_path: Path to the file to delete
+        context_db: Context database name
+    
+    Returns:
+        Number of chunks deleted
+    """
+    engine = get_engine(context_db)
+    
+    # Delete chunks
+    deleted_chunks = delete_chunks_by_path(engine, file_path)
+    
+    # Delete file index entry
+    delete_file_index(engine, file_path)
+    
+    print(f"[info] Deleted {deleted_chunks} chunks and file index for {file_path}")
+    
+    return deleted_chunks
+
+
+def list_files_in_context(
+    context_db: str = DEFAULT_CONTEXT_DB,
+) -> List[Tuple[str, str, str]]:
+    """List all files in a context database.
+    
+    Returns:
+        List of (path, sha256, updated_at) tuples
+    """
+    engine = get_engine(context_db)
+    with engine.begin() as conn:
+        rows = conn.execute(
+            text("SELECT path, sha256, updated_at FROM file_index ORDER BY updated_at DESC")
+        )
+        return [(r[0], r[1], r[2]) for r in rows]
+
+
+# Import text at the end to avoid circular import issues
+from sqlalchemy import text
+
+
+if __name__ == "__main__":
+    # CLI interface
+    import argparse
+    
+    parser = argparse.ArgumentParser(description="Enhanced upsert for Resume Context Builder")
+    parser.add_argument("markdown_dir", help="Directory containing markdown files")
+    parser.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB, 
+                        help="Context database name (default: default)")
+    parser.add_argument("--delete", "-d", metavar="FILE",
+                        help="Delete a specific file from context")
+    parser.add_argument("--list-contexts", action="store_true",
+                        help="List all context databases")
+    parser.add_argument("--list-files", action="store_true",
+                        help="List files in the context")
+    parser.add_argument("--delete-context", metavar="NAME",
+                        help="Delete an entire context database")
+    parser.add_argument("--decay", metavar="DAYS", type=int,
+                        help="Apply information decay (remove data older than N days)")
+    parser.add_argument("--dry-run", action="store_true",
+                        help="Show what would be deleted without actually deleting")
+    
+    args = parser.parse_args()
+    
+    if args.list_contexts:
+        contexts = list_context_databases()
+        print("Available context databases:")
+        for ctx in contexts:
+            marker = " *" if ctx == DEFAULT_CONTEXT_DB else ""
+            print(f"  - {ctx}{marker}")
+        sys.exit(0)
+    
+    if args.delete_context:
+        if args.delete_context == DEFAULT_CONTEXT_DB:
+            print(f"Error: Cannot delete default context '{DEFAULT_CONTEXT_DB}'")
+            sys.exit(1)
+        if delete_context_database(args.delete_context):
+            print(f"Deleted context database: {args.delete_context}")
+        else:
+            print(f"Context not found: {args.delete_context}")
+        sys.exit(0)
+    
+    if args.list_files:
+        files = list_files_in_context(args.context)
+        print(f"Files in context '{args.context}':")
+        for path, sha, updated in files:
+            print(f"  - {path} (updated: {updated})")
+        sys.exit(0)
+    
+    if args.delete:
+        deleted = delete_file_from_context(args.delete, args.context)
+        print(f"Deleted {deleted} chunks from {args.delete}")
+        sys.exit(0)
+    
+    if args.decay:
+        engine = get_engine(args.context)
+        results = apply_information_decay(engine, max_age_days=args.decay, dry_run=args.dry_run)
+        print(f"Information decay ({args.decay} days):")
+        print(f"  Chunks: {results['chunks']}")
+        print(f"  File index: {results['file_index']}")
+        print(f"  Job runs: {results['job_runs']}")
+        print(f"  Feedback: {results['feedback']}")
+        if args.dry_run:
+            print("(Dry run - nothing was actually deleted)")
+        sys.exit(0)
+    
+    # Default: upsert files
+    root = Path(args.markdown_dir).expanduser().resolve()
+    if not root.exists():
+        print(f"Error: Directory not found: {root}")
+        sys.exit(1)
+    
+    md_files = sorted(root.rglob("*.md"))
+    if not md_files:
+        print(f"No markdown files found in {root}")
+        sys.exit(0)
+    
+    count = upsert_markdown_files(md_files, context_db=args.context)
+    print(f"Upserted {count} chunk(s) from {len(md_files)} file(s) into context '{args.context}'")
diff --git a/pyproject.toml b/pyproject.toml
index b3d82d5..fd224c4 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -4,7 +4,7 @@ build-backend = "hatchling.build"
 
 [project]
 name = "context-packager"
-version = "0.2.4"
+version = "0.2.26"
 description = "Convert documents to Markdown (MarkItDown), package with Repomix, and serve a Streamlit UI for copyable LLM context."
 readme = "README.md"
 requires-python = ">=3.10"
@@ -21,6 +21,7 @@ dependencies = [
   "pynndescent==0.5.13",
   "APScheduler==3.10.4",
   "pyvis==0.3.2",
+  "streamlit-agraph==0.0.45",
 ]
 
 [project.urls]
@@ -35,7 +36,7 @@ context-scheduler = "resume_ui.scheduler_cli:main"
 
 [tool.hatch.build.targets.wheel]
 packages = ["resume_ui", "hr_tools", "kb", "context_packager_data"]
-force-include = { "hr_tools/instruction.md" = "hr_tools/instruction.md" }
+force-include = { "hr_tools/instruction.md" = "hr_tools/instruction.md", "kb/tuning_model.json" = "kb/tuning_model.json" }
 
 [tool.hatch.build.targets.sdist]
 exclude = [
diff --git a/rcb b/rcb
new file mode 100755
index 0000000..abb0276
--- /dev/null
+++ b/rcb
@@ -0,0 +1,317 @@
+#!/usr/bin/env python3
+"""
+Resume Context Builder - Enhanced CLI
+
+Features:
+- Multiple context databases
+- Fixed file deletion
+- Information decay
+- Time range filtering
+"""
+
+import argparse
+import sys
+from pathlib import Path
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent.parent))
+
+from kb.db_enhanced import (
+    list_context_databases,
+    delete_context_database,
+    apply_information_decay,
+    get_decay_log,
+    get_time_range,
+    DEFAULT_CONTEXT_DB,
+    get_engine,
+)
+from kb.upsert_enhanced import (
+    upsert_markdown_files,
+    delete_file_from_context,
+    list_files_in_context,
+)
+from kb.search_enhanced import search_with_time_filter
+
+
+def cmd_list_contexts(args):
+    """List all context databases."""
+    contexts = list_context_databases()
+    print("Available context databases:")
+    print("-" * 40)
+    for ctx in contexts:
+        marker = " *" if ctx == DEFAULT_CONTEXT_DB else ""
+        print(f"  - {ctx}{marker}")
+    if not contexts:
+        print("  (none found)")
+
+
+def cmd_create_context(args):
+    """Create a new context database."""
+    # Just initialize by getting engine
+    engine = get_engine(args.name)
+    print(f"Created context database: {args.name}")
+
+
+def cmd_delete_context(args):
+    """Delete a context database."""
+    if args.name == DEFAULT_CONTEXT_DB:
+        print(f"Error: Cannot delete default context '{DEFAULT_CONTEXT_DB}'")
+        sys.exit(1)
+    
+    if delete_context_database(args.name):
+        print(f"Deleted context database: {args.name}")
+    else:
+        print(f"Context not found: {args.name}")
+        sys.exit(1)
+
+
+def cmd_list_files(args):
+    """List files in a context."""
+    files = list_files_in_context(args.context)
+    print(f"Files in context '{args.context}':")
+    print("-" * 60)
+    if not files:
+        print("  (no files)")
+        return
+    for path, sha, updated in files:
+        print(f"  - {path}")
+        print(f"    Updated: {updated}")
+        print()
+
+
+def cmd_upsert(args):
+    """Upsert markdown files into a context."""
+    root = Path(args.dir).expanduser().resolve()
+    if not root.exists():
+        print(f"Error: Directory not found: {root}")
+        sys.exit(1)
+    
+    md_files = sorted(root.rglob("*.md"))
+    if not md_files:
+        print(f"No markdown files found in {root}")
+        sys.exit(0)
+    
+    print(f"Found {len(md_files)} markdown files")
+    print(f"Upserting into context: {args.context}")
+    print("-" * 40)
+    
+    def progress(current, total, path):
+        pct = (current / total) * 100
+        print(f"  [{pct:5.1f}%] {path.name}")
+    
+    count = upsert_markdown_files(
+        md_files,
+        context_db=args.context,
+        progress_cb=progress if args.verbose else None
+    )
+    
+    print("-" * 40)
+    print(f"Upserted {count} chunks from {len(md_files)} files")
+
+
+def cmd_delete_file(args):
+    """Delete a file from a context."""
+    deleted = delete_file_from_context(args.path, args.context)
+    print(f"Deleted {deleted} chunks from {args.path}")
+
+
+def cmd_decay(args):
+    """Apply information decay."""
+    engine = get_engine(args.context)
+    
+    print(f"Applying information decay ({args.days} days)")
+    print(f"Context: {args.context}")
+    if args.dry_run:
+        print("Mode: DRY RUN (no actual deletions)")
+    print("-" * 40)
+    
+    results = apply_information_decay(engine, max_age_days=args.days, dry_run=args.dry_run)
+    
+    print(f"  Chunks deleted:      {results['chunks']}")
+    print(f"  File index entries:  {results['file_index']}")
+    print(f"  Job runs deleted:    {results['job_runs']}")
+    print(f"  Feedback deleted:    {results['feedback']}")
+    
+    if args.dry_run:
+        print("\n(Dry run - nothing was actually deleted)")
+    else:
+        print("\nDecay applied successfully")
+
+
+def cmd_decay_log(args):
+    """Show decay operation history."""
+    engine = get_engine(args.context)
+    logs = get_decay_log(engine, limit=args.limit)
+    
+    print(f"Recent decay operations for '{args.context}':")
+    print("-" * 60)
+    
+    if not logs:
+        print("  (no decay operations recorded)")
+        return
+    
+    for action, details, executed_at in logs:
+        print(f"  [{executed_at}]")
+        print(f"  Action: {action}")
+        print(f"  Details: {details}")
+        print()
+
+
+def cmd_search(args):
+    """Search with time range filtering."""
+    print(f"Searching: '{args.query}'")
+    print(f"Context: {args.context}")
+    print(f"Time range: {args.time_range}")
+    print(f"Top-K: {args.top_k}")
+    print("-" * 60)
+    
+    results = search_with_time_filter(
+        args.query,
+        time_range=args.time_range,
+        top_k=args.top_k,
+        context_db=args.context
+    )
+    
+    if not results:
+        print("No results found")
+        return
+    
+    for i, (cid, score, path, cname, snippet, _) in enumerate(results, 1):
+        print(f"\n{i}. Score: {score:.4f} | Chunk: {cname}")
+        print(f"   Path: {path}")
+        print(f"   Snippet: {snippet[:200]}...")
+    
+    print(f"\nTotal: {len(results)} results")
+
+
+def cmd_time_range(args):
+    """Show time range conversion."""
+    since, until = get_time_range(args.range_str)
+    print(f"Time range: {args.range_str}")
+    print(f"  Since: {since or '(no limit)'}")
+    print(f"  Until: {until or '(now)'}")
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Resume Context Builder - Enhanced CLI",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+  # List all context databases
+  %(prog)s contexts list
+  
+  # Create a new context
+  %(prog)s contexts create my-project
+  
+  # Upsert files into a context
+  %(prog)s upsert /path/to/markdown --context my-project
+  
+  # List files in a context
+  %(prog)s files list --context my-project
+  
+  # Delete a file from context
+  %(prog)s files delete /path/to/file.md --context my-project
+  
+  # Search with time filter (last 3 months)
+  %(prog)s search "python developer" --time-range 3m
+  
+  # Apply information decay (remove data older than 1 year)
+  %(prog)s decay 365 --dry-run
+  
+  # Delete an entire context
+  %(prog)s contexts delete my-project
+        """
+    )
+    
+    subparsers = parser.add_subparsers(dest="command", help="Commands")
+    
+    # Context commands
+    context_parser = subparsers.add_parser("contexts", help="Manage context databases")
+    context_sub = context_parser.add_subparsers(dest="context_cmd")
+    
+    context_list = context_sub.add_parser("list", help="List all contexts")
+    context_list.set_defaults(func=cmd_list_contexts)
+    
+    context_create = context_sub.add_parser("create", help="Create a new context")
+    context_create.add_argument("name", help="Context name")
+    context_create.set_defaults(func=cmd_create_context)
+    
+    context_delete = context_sub.add_parser("delete", help="Delete a context")
+    context_delete.add_argument("name", help="Context name")
+    context_delete.set_defaults(func=cmd_delete_context)
+    
+    # File commands
+    file_parser = subparsers.add_parser("files", help="Manage files in context")
+    file_sub = file_parser.add_subparsers(dest="file_cmd")
+    
+    file_list = file_sub.add_parser("list", help="List files in context")
+    file_list.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB, help="Context name")
+    file_list.set_defaults(func=cmd_list_files)
+    
+    file_delete = file_sub.add_parser("delete", help="Delete a file from context")
+    file_delete.add_argument("path", help="File path")
+    file_delete.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB, help="Context name")
+    file_delete.set_defaults(func=cmd_delete_file)
+    
+    # Upsert command
+    upsert_parser = subparsers.add_parser("upsert", help="Upsert markdown files")
+    upsert_parser.add_argument("dir", help="Directory containing markdown files")
+    upsert_parser.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB, help="Context name")
+    upsert_parser.add_argument("--verbose", "-v", action="store_true", help="Show progress")
+    upsert_parser.set_defaults(func=cmd_upsert)
+    
+    # Decay commands
+    decay_parser = subparsers.add_parser("decay", help="Information decay management")
+    decay_sub = decay_parser.add_subparsers(dest="decay_cmd")
+    
+    decay_apply = decay_sub.add_parser("apply", help="Apply decay (remove old data)")
+    decay_apply.add_argument("days", type=int, help="Maximum age in days")
+    decay_apply.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB, help="Context name")
+    decay_apply.add_argument("--dry-run", "-n", action="store_true", help="Show what would be deleted")
+    decay_apply.set_defaults(func=cmd_decay)
+    
+    decay_log = decay_sub.add_parser("log", help="Show decay history")
+    decay_log.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB, help="Context name")
+    decay_log.add_argument("--limit", "-l", type=int, default=10, help="Number of entries")
+    decay_log.set_defaults(func=cmd_decay_log)
+    
+    # Search command
+    search_parser = subparsers.add_parser("search", help="Search with time filtering")
+    search_parser.add_argument("query", help="Search query")
+    search_parser.add_argument("--context", "-c", default=DEFAULT_CONTEXT_DB, help="Context name")
+    search_parser.add_argument("--time-range", "-t", default="all",
+                               choices=["1d", "1w", "1m", "3m", "6m", "1y", "all"],
+                               help="Time range filter")
+    search_parser.add_argument("--top-k", "-k", type=int, default=5, help="Number of results")
+    search_parser.set_defaults(func=cmd_search)
+    
+    # Time range utility
+    time_parser = subparsers.add_parser("time-range", help="Show time range conversion")
+    time_parser.add_argument("range_str", help="Time range string (e.g., 1d, 3m, 1y)")
+    time_parser.set_defaults(func=cmd_time_range)
+    
+    args = parser.parse_args()
+    
+    if not args.command:
+        parser.print_help()
+        sys.exit(1)
+    
+    # Handle sub-subcommands
+    if args.command == "contexts" and not args.context_cmd:
+        context_parser.print_help()
+        sys.exit(1)
+    
+    if args.command == "files" and not args.file_cmd:
+        file_parser.print_help()
+        sys.exit(1)
+    
+    if args.command == "decay" and not args.decay_cmd:
+        decay_parser.print_help()
+        sys.exit(1)
+    
+    args.func(args)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/requirements.txt b/requirements.txt
index db16804..858f6ae 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,10 +1,24 @@
-markitdown[all]==0.1.3
+# Core dependencies
 repomix==0.3.4
 streamlit==1.50.0
 tiktoken==0.11.0
-scikit-learn
-pynndescent
+
+# Database
+SQLAlchemy==2.0.35
+psycopg[binary,pool]==3.2.1
+
+# ML / Search
+scikit-learn==1.5.2
+pynndescent==0.5.13
 numpy
-sqlalchemy
-streamlit-agraph
-thefuzz[speedup]
+scipy
+
+# Scheduling
+APScheduler==3.10.4
+
+# Visualization
+pyvis==0.3.2
+
+# Document conversion
+markitdown[all]==0.1.3
+pdfminer.six==20250506
diff --git a/resume_ui/app.py b/resume_ui/app.py
index facce94..15f120f 100644
--- a/resume_ui/app.py
+++ b/resume_ui/app.py
@@ -17,81 +17,176 @@ import streamlit.components.v1 as components
 from hr_tools.pdf_to_md import convert_pdfs_to_markdown
 from hr_tools.package_context import package_markdown_directory
 from kb.upsert import upsert_markdown_files
-from kb.db import get_engine, fetch_all_chunks, count_chunks, fetch_chunks, delete_chunks_by_ids, fetch_chunk_by_id, STATE_DIR, get_database_url
+from kb.db import get_engine, fetch_all_chunks, count_chunks, fetch_chunks, delete_chunks_by_ids, fetch_chunk_by_id, STATE_DIR, get_database_url, record_feedback, record_search_history
 from kb.search import HybridSearcher
+from kb.tuning import get_optimizer
 from kb.artifacts import load_artifacts
 from kb.db import fetch_meta_nodes_for_chunks
 
 try:
-    from pyvis.network import Network
-except ImportError:
-    Network = None
+    import streamlit_agraph
+    # Attempt to get the raw component function
+    if hasattr(streamlit_agraph, "_agraph"):
+        _raw_agraph_component = streamlit_agraph._agraph
+    else:
+        # Re-declare it if we can't find it (fallback hack)
+        import os
+        import streamlit.components.v1 as components
+        parent_dir = os.path.dirname(os.path.abspath(streamlit_agraph.__file__))
+        build_dir = os.path.join(parent_dir, "frontend/build")
+        _raw_agraph_component = components.declare_component("agraph", path=build_dir)
+
+    from streamlit_agraph import Node, Edge, Config
+    
+    # Redefine agraph wrapper to support key explicitly
+    def agraph(nodes, edges, config, **kwargs):
+        # Standard serialization logic from the original library
+        node_ids = [node.id for node in nodes]
+        if len(node_ids) > len(set(node_ids)):
+            st.warning("Duplicated node IDs exist.")
+        nodes_data = [node.to_dict() for node in nodes]
+        edges_data = [edge.to_dict() for edge in edges]
+        config_json = json.dumps(config.__dict__)
+        data_dict = {"nodes": nodes_data, "edges": edges_data}
+        data_json = json.dumps(data_dict)
+        return _raw_agraph_component(data=data_json, config=config_json, **kwargs)
+
+except Exception:
+    agraph = None
 
-def render_interactive_graph(searcher, cids: list[int]):
-    if not Network:
-        st.warning("pyvis not installed")
+def render_interactive_graph(searcher, cids: list[int], clusters: list[dict] | None = None):
+    if not agraph:
+        st.warning("streamlit-agraph not installed")
         return None
 
     # Map cid -> internal idx
     cid_to_idx = {c: i for i, c in enumerate(searcher.ids)}
     
+    # Map cid -> cluster label/color
+    cid_to_cluster = {}
+    if clusters:
+        for i, c in enumerate(clusters):
+            label = c["label"]
+            for cid in c["cids"]:
+                cid_to_cluster[cid] = (label, i)
+    
     # Fetch Meta Info
     meta_map = fetch_meta_nodes_for_chunks(get_engine(), cids)
     
-    net = Network(height="500px", width="100%", bgcolor="#ffffff", font_color="black")
-    net.force_atlas_2based()
-    
+    nodes = []
+    edges = []
     added_nodes = set()
     
+    # Define a color palette for clusters
+    cluster_colors = ["#4b8fea", "#ff6b6b", "#51cf66", "#fcc419", "#ff922b", "#845ef7", "#e64980", "#20c997"]
+    
     for cid in cids:
-        if cid not in added_nodes:
+        if str(cid) not in added_nodes:
             idx = cid_to_idx.get(cid)
             label = f"Doc {cid}"
             title = "Unknown"
+            
+            # Determine color and group from cluster
+            color = "#4b8fea" # Default blue
+            group = "default"
+            if cid in cid_to_cluster:
+                c_label, c_idx = cid_to_cluster[cid]
+                color = cluster_colors[c_idx % len(cluster_colors)]
+                group = c_label
+                title = f"Cluster: {c_label}\n"
+
             if idx is not None:
                 path, name = searcher.meta[idx]
-                label = name
-                title = f"{path}\n{name}"
+                content = searcher.texts[idx] if idx < len(searcher.texts) else ""
+                label = _friendly_title(path, name, content)
+                # Truncate label if too long for vis
+                if len(label) > 15: label = label[:15] + ".."
+                title += f"{path}\n{name}"
             
-            # Use smaller nodes for docs
-            net.add_node(f"doc_{cid}", label=label, title=title, size=15, color="#4b8fea", shape="box")
-            added_nodes.add(f"doc_{cid}")
+            nodes.append(Node(
+                id=str(cid), # ID must be string
+                label=label,
+                title=title,
+                size=20,
+                color=color,
+                group=group,
+                shape="box"
+            ))
+            added_nodes.add(str(cid))
             
             # Add Meta Nodes
             metas = meta_map.get(cid, [])
             for m_type, m_name in metas:
                 m_id = f"meta_{m_type}_{m_name}"
                 if m_id not in added_nodes:
-                    color = "#ff6b6b" if m_type == "topic" else ("#51cf66" if m_type == "entity" else "#fcc419")
-                    net.add_node(m_id, label=m_name, title=f"{m_type}: {m_name}", size=25, color=color, shape="ellipse")
+                    m_color = "#e9ecef" # Neutral for meta nodes
+                    # Meta node
+                    nodes.append(Node(
+                        id=m_id,
+                        label=m_name,
+                        title=f"{m_type}: {m_name}",
+                        size=10,
+                        color=m_color,
+                        shape="ellipse"
+                    ))
                     added_nodes.add(m_id)
                 
                 # Edge Doc -> Meta
-                net.add_edge(f"doc_{cid}", m_id, color="#e0e0e0")
+                edges.append(Edge(
+                    source=str(cid),
+                    target=m_id,
+                    color="#e0e0e0"
+                ))
+
+    config = Config(
+        width="100%",
+        height=500,
+        directed=False, 
+        physics=True, 
+        hierarchical=False,
+        nodeHighlightBehavior=True,
+        highlightColor="#F7A7A6",
+        collapsible=False
+    )
 
-    # Generate graph html
+    selection = agraph(nodes=nodes, edges=edges, config=config, key="interactive_kb_graph")
+    return selection
+
+def _friendly_title(path: str, chunk_name: str, content: str = "") -> str:
+    # Prefer first Markdown heading if present
     try:
-        # Save to temp file and read back or use tempfile
-        # pyvis write_html writes a full page.
-        # We can also get the html string directly if we use generate_html() but it needs a name.
-        tmp_name = f"graph_{uuid.uuid4().hex}.html"
-        path = Path(tempfile.gettempdir()) / tmp_name
-        net.save_graph(str(path))
-        html_content = path.read_text(encoding="utf-8")
-        # Cleanup
-        try:
-            path.unlink()
-        except Exception:
-            pass
-        
-        components.html(html_content, height=520)
-        return None # Pyvis in streamlit doesn't easily return clicked events without custom component
-    except Exception as e:
-        st.error(f"Graph generation error: {e}")
-        return None
+        if content:
+            for line in content.splitlines():
+                s = line.strip()
+                if not s: continue
+                # Skip lines that look like sentence fragments (start with lowercase)
+                if s[0].islower(): continue
+                
+                if s.startswith("# "):
+                    return s[2:].strip()
+                if s.startswith("## "):
+                    return s[3:].strip()
+                if s and not s.startswith("#") and len(s) > 8:
+                    # heuristic: must not be too long to be a title, and shouldn't look like a sentence
+                    if len(s) < 80 and not s.endswith((".", ",", ":", ";")):
+                        return s
+    except Exception:
+        pass
+    # Fallback to filename without noisy tokens
+    name = Path(path).name
+    base = name.rsplit(".", 1)[0]
+    cleaned = base.replace("_", " ").replace("-", " ")
+    cleaned = " ".join(part for part in cleaned.split() if not part.isupper() or len(part) <= 5)
+    return cleaned.strip() or chunk_name or name
+
+import importlib.metadata
+try:
+    __version__ = importlib.metadata.version("context-packager")
+except Exception:
+    __version__ = "0.0.0"
 
 st.set_page_config(page_title="Context Packager", layout="wide", initial_sidebar_state="collapsed")
-st.title("Context Packager")
+st.title(f"Context Packager v{__version__}")
 
 # Defaults designed to be portable on any machine
 default_pdf_dir = ""
@@ -303,24 +398,58 @@ with st.sidebar:
         except Exception as e:
             st.error(f"Failed to save settings: {e}")
     st.caption("KB search configuration")
-    kb_top_k = st.number_input("Max results", min_value=1, max_value=50, step=1, key="kb_top_k")
-    kb_min_score = st.slider("Minimum score", min_value=0.0, max_value=1.0, step=0.005, key="kb_min_score")
-    kb_neighbors = st.number_input("Neighbors to include (per match)", min_value=0, max_value=10, step=1, key="kb_neighbors")
+    auto_tune = st.toggle("Auto-Tune (MCTS-RL)", value=True, help="Automatically adjust parameters based on user feedback using a tree search optimizer.")
+    
+    if auto_tune:
+        st.caption("Auto-Tune Active. Manual overrides disabled.")
+        # We still define variables for scope but don't show widgets or disable them
+        # Just use placeholders or last knowns, the actual search logic will use optimizer
+        kb_top_k = 5
+        kb_min_score = 0.005
+        kb_neighbors = 0
+    else:
+        kb_top_k = st.number_input("Max results", min_value=1, max_value=50, step=1, key="kb_top_k")
+        kb_min_score = st.slider("Minimum score", min_value=0.0, max_value=1.0, step=0.005, key="kb_min_score")
+        kb_neighbors = st.number_input("Neighbors to include (per match)", min_value=0, max_value=10, step=1, key="kb_neighbors")
+        
     kb_sequence = st.checkbox("Preserve document sequence (group by file, in order)", key="kb_sequence")
-    # Advanced retrieval tuning
-    kb_use_ann = st.checkbox("Use ANN (NN-Descent) for candidates", key="kb_use_ann")
-    kb_cand_mult = st.number_input("Candidate multiplier", min_value=1, max_value=20, step=1, key="kb_cand_mult")
-    kb_bm25_weight = st.slider("BM25 weight (0=TF-IDF, 1=BM25)", min_value=0.0, max_value=1.0, step=0.05, key="kb_bm25_weight")
-    kb_lsa_weight = st.slider("LSA weight (SVD cosine)", min_value=0.0, max_value=1.0, step=0.05, key="kb_lsa_weight")
-    kb_tfidf_metric = st.selectbox("TF-IDF similarity metric", options=["cosine", "l2"], key="kb_tfidf_metric")
-    kb_ann_weight = st.slider("ANN (NN-Descent) weight", min_value=0.0, max_value=1.0, step=0.05, key="kb_ann_weight")
-    mmr_div = st.checkbox("Diversify results (MMR)", key="kb_mmr_diversify")
-    mmr_lambda = st.slider("MMR lambda (relevance vs diversity)", min_value=0.0, max_value=1.0, step=0.05, key="kb_mmr_lambda")
-    phrase_boost = st.slider("Quoted phrase boost", min_value=0.0, max_value=0.5, step=0.05, key="kb_phrase_boost")
-    enable_rare = st.checkbox("Filter candidates by rare terms", key="kb_enable_rare_filter")
-    rare_idf_th = st.number_input("Rare term IDF threshold", min_value=0.0, max_value=20.0, step=0.5, key="kb_rare_idf_threshold")
+    
+    if not auto_tune:
+        # Advanced retrieval tuning
+        kb_use_ann = st.checkbox("Use ANN (NN-Descent) for candidates", key="kb_use_ann")
+        kb_cand_mult = st.number_input("Candidate multiplier", min_value=1, max_value=20, step=1, key="kb_cand_mult")
+        kb_bm25_weight = st.slider("BM25 weight (0=TF-IDF, 1=BM25)", min_value=0.0, max_value=1.0, step=0.05, key="kb_bm25_weight")
+        kb_lsa_weight = st.slider("LSA weight (SVD cosine)", min_value=0.0, max_value=1.0, step=0.05, key="kb_lsa_weight")
+        kb_tfidf_metric = st.selectbox("TF-IDF similarity metric", options=["cosine", "l2"], key="kb_tfidf_metric")
+        kb_ann_weight = st.slider("ANN (NN-Descent) weight", min_value=0.0, max_value=1.0, step=0.05, key="kb_ann_weight")
+        mmr_div = st.checkbox("Diversify results (MMR)", key="kb_mmr_diversify")
+        mmr_lambda = st.slider("MMR lambda (relevance vs diversity)", min_value=0.0, max_value=1.0, step=0.05, key="kb_mmr_lambda")
+        phrase_boost = st.slider("Quoted phrase boost", min_value=0.0, max_value=0.5, step=0.05, key="kb_phrase_boost")
+        enable_rare = st.checkbox("Filter candidates by rare terms", key="kb_enable_rare_filter")
+        rare_idf_th = st.number_input("Rare term IDF threshold", min_value=0.0, max_value=20.0, step=0.5, key="kb_rare_idf_threshold")
 
     st.divider()
+    
+    # Auto-Tune Active Params Display in Sidebar
+    if auto_tune and "kb_searcher" in st.session_state:
+        # Check if we have recent params from search
+        # We can't easily get the *last used* params unless stored in session
+        # But we can ask the optimizer for current suggestion
+        try:
+            opt = get_optimizer()
+            # This might differ slightly if exploration happened, but gives user an idea
+            # Ideally we capture this from the main search execution
+            if "last_search_params" in st.session_state:
+                disp_params = st.session_state["last_search_params"]
+            else:
+                disp_params = opt.suggest_parameters(exploration_prob=0.0) # Show exploitation params as baseline
+            
+            with st.expander("Auto-Tune Parameters", expanded=False):
+                st.json(disp_params)
+                st.caption("These parameters are automatically adjusted based on your feedback.")
+        except Exception:
+            pass
+
     st.caption("Instructions (optional)")
 
     @st.dialog("Edit instructions")
@@ -409,7 +538,7 @@ def render_copy_button(label: str, text: str, height: int = 110, disabled: bool
     components.html(html, height=height)
 
 # Tabs
-home_tab, manage_tab = st.tabs(["Home", "Manage knowledge"])
+home_tab, package_tab, manage_tab = st.tabs(["Search", "Package", "Manage knowledge"])
 
 with home_tab:
     # Global search bar pinned at top (independent form)
@@ -483,24 +612,50 @@ with home_tab:
                 # Effective threshold with base floor (prevents persisted 0.0 from leaking)
                 _saved_ms = float(st.session_state.get("kb_min_score", 0.005))
                 min_score = max(0.005, _saved_ms)
+                
+                # Parameters: either from Auto-Tune (Optimizer) or Manual
+                params = {}
+                if auto_tune:
+                    opt = get_optimizer()
+                    params = opt.suggest_parameters()
+                    st.session_state["last_search_params"] = params
+                else:
+                    params = {
+                        "kb_top_k": top_k,
+                        "kb_min_score": min_score,
+                        "kb_neighbors": int(st.session_state.get("kb_neighbors", 0)),
+                        "kb_bm25_weight": float(st.session_state.get("kb_bm25_weight", 0.6)),
+                        "kb_cand_mult": int(st.session_state.get("kb_cand_mult", 3)),
+                        "kb_lsa_weight": float(st.session_state.get("kb_lsa_weight", 0.2)),
+                        "kb_ann_weight": float(st.session_state.get("kb_ann_weight", 0.0)),
+                        "kb_mmr_diversify": bool(st.session_state.get("kb_mmr_diversify", True)),
+                        "kb_mmr_lambda": float(st.session_state.get("kb_mmr_lambda", 0.2)),
+                        "kb_phrase_boost": float(st.session_state.get("kb_phrase_boost", 0.1)),
+                        "kb_enable_rare_filter": bool(st.session_state.get("kb_enable_rare_filter", True)),
+                        "kb_rare_idf_threshold": float(st.session_state.get("kb_rare_idf_threshold", 3.0)),
+                    }
+
                 results = searcher.search(
                     st.session_state["q_top"],
-                    top_k=top_k,
-                    neighbors=int(st.session_state.get("kb_neighbors", 0)),
+                    top_k=params.get("kb_top_k", 5),
+                    neighbors=params.get("kb_neighbors", 0),
                     sequence=bool(st.session_state.get("kb_sequence", True)),
                     use_ann=bool(st.session_state.get("kb_use_ann", True)),
-                    bm25_weight=float(st.session_state.get("kb_bm25_weight", 0.6)),
-                    cand_multiplier=int(st.session_state.get("kb_cand_mult", 3)),
-                    lsa_weight=float(st.session_state.get("kb_lsa_weight", 0.2)),
+                    bm25_weight=params.get("kb_bm25_weight", 0.45),
+                    cand_multiplier=params.get("kb_cand_mult", 5),
+                    lsa_weight=params.get("kb_lsa_weight", 0.2),
                     tfidf_metric=str(st.session_state.get("kb_tfidf_metric", "cosine")),
-                    ann_weight=float(st.session_state.get("kb_ann_weight", 0.0)),
-                    min_score=float(min_score),
-                    mmr_diversify=bool(st.session_state.get("kb_mmr_diversify", True)),
-                    mmr_lambda=float(st.session_state.get("kb_mmr_lambda", 0.2)),
-                    phrase_boost=float(st.session_state.get("kb_phrase_boost", 0.1)),
-                    enable_rare_term_filter=bool(st.session_state.get("kb_enable_rare_filter", True)),
-                    rare_idf_threshold=float(st.session_state.get("kb_rare_idf_threshold", 3.0)),
+                    ann_weight=params.get("kb_ann_weight", 0.15),
+                    min_score=params.get("kb_min_score", 0.005),
+                    mmr_diversify=params.get("kb_mmr_diversify", True),
+                    mmr_lambda=params.get("kb_mmr_lambda", 0.2),
+                    phrase_boost=params.get("kb_phrase_boost", 0.1),
+                    enable_rare_term_filter=params.get("kb_enable_rare_filter", True),
+                    rare_idf_threshold=params.get("kb_rare_idf_threshold", 3.0),
                 )
+                
+                # Log search history for optimization loop
+                record_search_history(st.session_state["q_top"], params, len(results))
                 # filter by minimum score (defensive)
                 results = [r for r in results if r[1] >= min_score]
                 # Treat as empty if best remaining score is below threshold
@@ -518,7 +673,10 @@ with home_tab:
                 else:
                     sections = []
                     for cid, score, path, cname, snippet, full_text in results:
-                        header = f"{path} :: {cname} ‚Äî relevancy score {score:.3f}"
+                        clean_name = Path(path).name
+                        if clean_name.lower().endswith(".md"):
+                            clean_name = clean_name[:-3]
+                        header = f"{clean_name} :: {cname} ‚Äî relevancy score {score:.3f}"
                         sections.append(f"{header}\n\n{full_text.strip()}")
                     aggregated = "\n\n---\n\n".join(sections)
 
@@ -547,56 +705,6 @@ with home_tab:
             st.subheader("Search results")
             # Collapsible, scrollable panels per result with selection checkboxes
             results_list = st.session_state.get("kb_results_list") or []
-            
-            # Contextual Ambiguity: Auto-Clustering
-            filtered_results = results_list
-            if results_list and "kb_searcher" in st.session_state:
-                try:
-                    searcher = st.session_state["kb_searcher"]
-                    cids = [r[0] for r in results_list]
-                    clusters = searcher.cluster_search_results(cids, n_clusters=5)
-                    
-                    if len(clusters) > 1:
-                        st.subheader("Context Clusters")
-                        st.caption("Results grouped by semantic similarity. Click a cluster to filter.")
-                        
-                        # Use columns or pills to select cluster
-                        # We use session state to track active filter
-                        if "cluster_filter" not in st.session_state:
-                            st.session_state["cluster_filter"] = "All"
-                            
-                        cols = st.columns(len(clusters) + 1)
-                        if cols[0].button("All", type="primary" if st.session_state["cluster_filter"] == "All" else "secondary", use_container_width=True):
-                            st.session_state["cluster_filter"] = "All"
-                            st.rerun()
-                            
-                        for i, c in enumerate(clusters):
-                            label = c["label"]
-                            count = c["count"]
-                            idx = i + 1
-                            if idx < len(cols):
-                                if cols[idx].button(f"{label} ({count})", type="primary" if st.session_state["cluster_filter"] == str(i) else "secondary", use_container_width=True):
-                                    st.session_state["cluster_filter"] = str(i)
-                                    st.rerun()
-                        
-                        # Apply filter
-                        if st.session_state["cluster_filter"] != "All":
-                            try:
-                                c_idx = int(st.session_state["cluster_filter"])
-                                if 0 <= c_idx < len(clusters):
-                                    cluster_cids = set(clusters[c_idx]["cids"])
-                                    filtered_results = [r for r in results_list if r[0] in cluster_cids]
-                                    render_results(filtered_results, key_suffix=f"_c{c_idx}")
-                                    # Skip default rendering
-                                    results_list = [] 
-                            except ValueError:
-                                pass
-                        else:
-                             render_results(results_list, key_suffix="_all")
-                             results_list = [] # Handled
-                except Exception as e:
-                    # Fallback to normal list
-                    pass
 
             # Define renderer to reuse
             def render_results(results_subset, key_suffix=""):
@@ -617,7 +725,10 @@ with home_tab:
                 if selected_results:
                     sections = []
                     for cid, score, path, cname, snippet, full_text in selected_results:
-                        header = f"{path} :: {cname} ‚Äî relevancy score {score:.3f}"
+                        clean_name = Path(path).name
+                        if clean_name.lower().endswith(".md"):
+                            clean_name = clean_name[:-3]
+                        header = f"{clean_name} :: {cname} ‚Äî relevancy score {score:.3f}"
                         sections.append(f"{header}\n\n{full_text.strip()}")
                     aggregated_sel = "\n\n---\n\n".join(sections)
                 else:
@@ -636,120 +747,197 @@ with home_tab:
                 
                 # List
                 for i, (cid, score, path, cname, snippet, full_text) in enumerate(results_subset):
-                    cols = st.columns([1, 24])
+                    # Layout: Checkbox | Content (Expandable) | Feedback
+                    # center align vertical for buttons
+                    cols = st.columns([1, 20, 2], vertical_alignment="center")
                     with cols[0]:
                         # Update global map on change
                         def update_sel(c=cid, k=f"kb_sel_{cid}{key_suffix}"):
                             selected_map[c] = st.session_state[k]
                             st.session_state["kb_results_selected"] = selected_map
                         
-                        st.checkbox("", value=bool(selected_map.get(cid, True)), key=f"kb_sel_{cid}{key_suffix}", on_change=update_sel)
+                        st.checkbox("Select", value=bool(selected_map.get(cid, True)), key=f"kb_sel_{cid}{key_suffix}", on_change=update_sel, label_visibility="collapsed")
+                    
                     with cols[1]:
-                        header = f"{path} :: {cname} ‚Äî score {score:.3f}"
+                        clean_name = Path(path).name
+                        if clean_name.lower().endswith(".md"):
+                            clean_name = clean_name[:-3]
+                        header = f"{clean_name} :: {cname} ‚Äî score {score:.3f}"
                         with st.expander(header, expanded=(i == 0)):
-                            st.markdown(f"**{path}**\n\n{snippet}\n\n---\n\n{full_text}")
+                            st.caption(f"Source: {path}")
+                            st.markdown(f"{snippet}\n\n---\n\n{full_text}")
+
+                    with cols[2]:
+                         # Feedback buttons
+                         # Use current query from state
+                         curr_q = st.session_state.get("q_top", "")
+                         # Use streamlit's feedback widget if available (v1.37+), or fallback
+                         try:
+                             fb_key = f"fb_{cid}{key_suffix}"
+                             # This widget returns 0 (thumbs down) or 1 (thumbs up) or None
+                             sentiment = st.feedback("thumbs", key=fb_key)
+                             if sentiment is not None:
+                                  score = 1.0 if sentiment == 1 else -1.0
+                                  record_feedback(curr_q, cid, score)
+                                  if auto_tune:
+                                      try:
+                                          opt = get_optimizer()
+                                          opt.update(params, score) 
+                                      except Exception:
+                                          pass
+                                  # st.toast not needed as widget state persists visual feedback
+                         except AttributeError:
+                             # Fallback for older streamlit
+                             if st.button("üëç", key=f"fb_up_{cid}{key_suffix}", help="More like this", use_container_width=True):
+                                  record_feedback(curr_q, cid, 1.0)
+                                  if auto_tune:
+                                      try:
+                                          opt = get_optimizer()
+                                          opt.update(params, 1.0) 
+                                      except Exception:
+                                          pass
+                                  st.toast("Recorded: More like this")
+                             if st.button("üëé", key=f"fb_dn_{cid}{key_suffix}", help="Less like this", use_container_width=True):
+                                  record_feedback(curr_q, cid, -1.0)
+                                  if auto_tune:
+                                      try:
+                                          opt = get_optimizer()
+                                          opt.update(params, -1.0)
+                                      except Exception:
+                                          pass
+                                  st.toast("Recorded: Less like this")
 
-            # If we have clusters, use tabs
+            # Contextual Ambiguity: Auto-Clustering
+            # We calculate clusters to color the graph, but do not filter by them in the list.
+            clusters = []
             if results_list and "kb_searcher" in st.session_state:
                 try:
                     searcher = st.session_state["kb_searcher"]
                     cids = [r[0] for r in results_list]
                     clusters = searcher.cluster_search_results(cids, n_clusters=5)
-                    
-                    if len(clusters) > 1:
-                        st.caption("Auto-detected Contexts:")
-                        tab_labels = ["All"] + [f"{c['label']} ({c['count']})" for c in clusters]
-                        tabs = st.tabs(tab_labels)
-                        
-                        with tabs[0]:
-                            render_results(results_list, key_suffix="_all")
-                            
-                        for i, cluster in enumerate(clusters):
-                            with tabs[i+1]:
-                                cluster_cids = set(cluster["cids"])
-                                subset = [r for r in results_list if r[0] in cluster_cids]
-                                render_results(subset, key_suffix=f"_c{i}")
-                        
-                        # Skip default rendering
-                        results_list = [] 
                 except Exception:
-                    # Fallback
                     pass
 
+            render_results(results_list, key_suffix="_all")
+
             # Graph Visualization (Interactive)
-            # Pyvis is interactive but doesn't return selection easily in Streamlit without component
-            if filtered_results or results_list:
-                 with st.expander("Visualize Meta-Graph (Interactive)", expanded=False):
+            if results_list:
+                 with st.expander("Visualize Meta-Graph (Interactive)", expanded=True):
                     try:
                         searcher = st.session_state["kb_searcher"]
-                        viz_cids = [r[0] for r in (filtered_results or results_list)[:30]] # Limit to 30 nodes
-                        render_interactive_graph(searcher, viz_cids)
-                    except Exception as e:
-                        st.error(f"Graph error: {e}")
+                        
+                        # Graph Exploration State
+                        # We use session state to track if we are "focused" on a node to explore its neighbors
+                        if "graph_focus_node" not in st.session_state:
+                            st.session_state["graph_focus_node"] = None
+                        
+                        viz_cids = []
+                        focus_node = st.session_state["graph_focus_node"]
+                        
+                        # If focused, we need to fetch neighbors. For now, we only support document-centric focus 
+                        # or simple meta-centric focus by finding connected chunks.
+                        if focus_node:
+                            st.caption(f"Focused on: {focus_node}")
+                            if st.button("Clear focus (Reset view)"):
+                                st.session_state["graph_focus_node"] = None
+                                st.rerun()
+                            
+                            if focus_node.isdigit():
+                                # Focused on a Doc
+                                viz_cids = [int(focus_node)]
+                                # Fallback to showing top results for context if needed, but strict focus is better
+                                # For now, just show top results but centered on the doc if possible?
+                                # Let's just default to top results for now as fallback
+                                viz_cids = [r[0] for r in results_list[:40]]
+                            elif focus_node.startswith("meta_"):
+                                # Focused on a Keyword -> Show all chunks connected to it
+                                try:
+                                    parts = focus_node.split("_", 2)
+                                    if len(parts) >= 3:
+                                        m_type, m_name = parts[1], parts[2]
+                                        # Filter current search results to those connected
+                                        viz_cids = [r[0] for r in results_list if m_name.lower() in r[5].lower()]
+                                        if not viz_cids:
+                                             viz_cids = [r[0] for r in results_list[:40]]
+                                except Exception:
+                                    viz_cids = [r[0] for r in results_list[:40]]
+                        else:
+                            viz_cids = [r[0] for r in results_list[:40]]
 
-            selected_map = st.session_state.get("kb_results_selected") or {}
-            # Build aggregated text from only selected items BEFORE rendering list so the button sits above
-            if results_list:
-                selected_results = []
-                for cid, score, path, cname, snippet, full_text in results_list:
-                    sel_key = f"kb_sel_{cid}"
-                    sel_val = bool(st.session_state.get(sel_key, selected_map.get(cid, True)))
-                    if sel_val:
-                        selected_results.append((cid, score, path, cname, snippet, full_text))
-                if selected_results:
-                    sections = []
-                    for cid, score, path, cname, snippet, full_text in selected_results:
-                        header = f"{path} :: {cname} ‚Äî relevancy score {score:.3f}"
-                        sections.append(f"{header}\n\n{full_text.strip()}")
-                    aggregated_sel = "\n\n---\n\n".join(sections)
-                    # Enforce token cap similar to packaging
-                    try:
-                        max_tok = int(st.session_state.get("max_tokens_config") or 0)
-                        enc_name = st.session_state.get("encoding_name", "o200k_base")
-                        if max_tok and max_tok > 0:
-                            enc = get_encoding(enc_name)
-                            toks = enc.encode(aggregated_sel)
-                            if len(toks) > max_tok:
-                                aggregated_sel = enc.decode(toks[:max_tok])
-                    except Exception:
-                        aggregated_sel = aggregated_sel
-                else:
-                    aggregated_sel = ""
-                # Copy selected button ABOVE the results list with selected count and tooltip
-                sel_count = len(selected_results)
-                total_count = len(results_list)
-                copy_cols = st.columns([2, 1])
-                with copy_cols[0]:
-                    render_copy_button(
-                        "Copy all selected results",
-                        aggregated_sel,
-                        height=80,
-                        disabled=(sel_count == 0),
-                        help_text="Copies only the selected results",
-                    )
-                with copy_cols[1]:
-                    st.caption(f"Selected {sel_count} of {total_count}")
-                st.caption("Select which results to include below.")
+                        selection = render_interactive_graph(searcher, viz_cids, clusters=clusters)
+                        
+                        # Handle selection drill-down
+                        if selection:
+                            st.divider()
+                            st.caption(f"Graph Selection: {selection}")
+                            
+                            # Determine if selection is a Doc or Meta Node
+                            # Doc IDs are numeric strings, Meta IDs start with "meta_"
+                            if selection.startswith("meta_"):
+                                # Show context for this keyword
+                                parts = selection.split("_", 2) # meta_type_name
+                                if len(parts) >= 3:
+                                    m_type, m_name = parts[1], parts[2]
+                                    st.markdown(f"**Context for '{m_name}'**")
+                                    
+                                    # Exploration Button
+                                    if st.button(f"Focus on '{m_name}'"):
+                                        st.session_state["graph_focus_node"] = selection
+                                        st.rerun()
+
+                                    # Find docs connected to this meta node in the current results
+                                    count = 0
+                                    for cid, score, path, cname, snippet, full_text in results_list:
+                                        if m_name.lower() in full_text.lower():
+                                            clean_name = Path(path).name
+                                            if clean_name.lower().endswith(".md"):
+                                                clean_name = clean_name[:-3]
+                                            header = f"{clean_name} :: {cname}"
+                                            
+                                            with st.expander(header, expanded=(count==0)):
+                                                st.caption(f"Source: {path}")
+                                                # Highlight the term
+                                                highlit = full_text.replace(m_name, f"**{m_name}**").replace(m_name.title(), f"**{m_name.title()}**")
+                                                # Show a snippet around the term (PREVIEW)
+                                                try:
+                                                    idx = highlit.lower().find(m_name.lower())
+                                                    start = max(0, idx - 100)
+                                                    end = min(len(highlit), idx + 300)
+                                                    st.markdown(f"...{highlit[start:end]}...")
+                                                except Exception:
+                                                    st.markdown(highlit[:400])
+                                                
+                                                # FULL CONTEXT OPTION
+                                                st.text_area(f"Full Content ({cname})", full_text, height=200, key=f"full_{cid}_{m_name}")
+                                                
+                                            count += 1
+                                            if count >= 10: break # Show more for context
+
+                            elif selection.isdigit():
+                                # Show Doc details
+                                cid = int(selection)
+                                # Find in results or fetch
+                                found = False
+                                for r in results_list:
+                                    if r[0] == cid:
+                                        st.markdown(f"**Selected Document: {r[2]}**")
+                                        st.text_area("Full Content", r[5], height=200)
+                                        found = True
+                                        break
+                                if not found:
+                                    # fetch fresh
+                                    row = fetch_chunk_by_id(get_engine(), cid)
+                                    if row:
+                                        st.markdown(f"**Selected Document: {row[1]}**")
+                                        st.text_area("Full Content", row[3], height=200)
 
-                # Now render list with checkboxes and update selection map (revert to previous full expander per item)
-                for i, (cid, score, path, cname, snippet, full_text) in enumerate(results_list):
-                    cols = st.columns([1, 24])
-                    with cols[0]:
-                        sel = st.checkbox("", value=bool(selected_map.get(cid, True)), key=f"kb_sel_{cid}")
-                        selected_map[cid] = bool(sel)
-                    with cols[1]:
-                        header = f"{path} :: {cname} ‚Äî relevancy score {score:.3f}"
-                        with st.expander(header, expanded=(i == 0)):
-                            st.markdown(f"**{path} :: {cname} ‚Äî relevancy score {score:.3f}**\n\n{full_text}")
-                # Persist selection state
-                st.session_state["kb_results_selected"] = dict(selected_map)
+                    except Exception as e:
+                        st.error(f"Graph error: {e}")
             else:
-                # Fallback to aggregated rendering
-                st.markdown(st.session_state["kb_results_agg"])
-        else:
-            st.info("No search results found")
+                st.info("No search results found")
 
     # Input and packaging live only on Home tab
+with package_tab:
     st.subheader("Input")
     uploaded_files = st.file_uploader("Upload files or ZIP", type=None, accept_multiple_files=True, key="home_file_uploader")
     fallback_dir_main = st.text_input("Or enter a directory path (optional)", value="", key="home_fallback_dir")
@@ -944,340 +1132,344 @@ with home_tab:
 with manage_tab:
     st.subheader("Manage knowledge base")
     engine = get_engine()
-    st.divider()
-    st.subheader("Enable continuous folder sync")
+    
+    tab_sched, tab_browse, tab_sys = st.tabs(["Scheduler", "Browse & Edit", "System"])
+
+    with tab_sched:
+        st.subheader("Enable continuous folder sync")
+
+        # Redesigned scheduler: single add-job form + run-now
+        with st.form("add_job_form", clear_on_submit=False):
+            col_a, col_b = st.columns([3, 2])
+            with col_a:
+                sync_folder = st.text_input("Folder to sync", key="sync_folder", placeholder="/path/to/folder")
+                if sync_folder and not os.path.isdir(sync_folder):
+                    st.caption("Invalid folder path")
+                md_out_target = st.text_input("Markdown output dir (optional)", key="sync_md_out")
+                if md_out_target and not os.path.isdir(md_out_target):
+                    st.caption("Invalid folder path")
+            with col_b:
+                schedule_mode = st.selectbox("Schedule", options=["Daily", "Every X minutes", "Weekly", "Monthly", "Cron (advanced)"], key="sync_mode")
+                interval_minutes = int(st.number_input("Interval (minutes)", min_value=1, step=1, key="sync_interval")) if schedule_mode == "Every X minutes" else 1440
+                cron_expr = st.text_input("Cron (min hr dom mon dow)", key="sync_cron") if schedule_mode == "Cron (advanced)" else st.session_state.get("sync_cron", "0 2 * * *")
+                if schedule_mode == "Daily":
+                    st.time_input("Time of day", key="sync_time")
+                elif schedule_mode == "Weekly":
+                    days_options = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
+                    st.multiselect("Days of week", options=days_options, key="sync_weekdays")
+                    st.time_input("Time of day", key="sync_time_weekly")
+                elif schedule_mode == "Monthly":
+                    st.number_input("Day of month", min_value=1, max_value=31, step=1, key="sync_dom")
+                    st.time_input("Time of day", key="sync_time_monthly")
+            col1, col2 = st.columns([1,1])
+            add_job_submit = col1.form_submit_button("Add job", use_container_width=True)
+            run_now_submit = col2.form_submit_button("Run now", use_container_width=True)
+
+        from apscheduler.schedulers.background import BackgroundScheduler
+        # ThreadPoolExecutor not needed for interface-only scheduler
+        from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
+        from apscheduler.triggers.interval import IntervalTrigger
+        from apscheduler.triggers.cron import CronTrigger
+        from apscheduler.triggers.date import DateTrigger
+        from resume_ui.scheduler_cli import _job_ingest as _job_ingest_fn
+        from kb.db import ensure_job, fetch_recent_runs, request_cancel_run
+
+        # Scheduler for UI (add/remove only, no execution)
+        def _get_scheduler_interface():
+            jobstores = {'default': SQLAlchemyJobStore(url=get_database_url())}
+            # No executors needed if we don't start it, but good to have defaults
+            job_defaults = {'coalesce': True, 'max_instances': 1, 'misfire_grace_time': 300}
+            return BackgroundScheduler(jobstores=jobstores, job_defaults=job_defaults)
+
+        job_id = "ui_continuous_sync"
+        if add_job_submit:
+            try:
+                # We do NOT start the scheduler in the UI process.
+                # We just add the job to the store; the external scheduler service picks it up.
+                sched = _get_scheduler_interface()
+                trigger = None
+                if schedule_mode == "Daily":
+                    t = st.session_state.get("sync_time", dt_time(2, 0))
+                    trigger = CronTrigger(hour=int(getattr(t, 'hour', 2)), minute=int(getattr(t, 'minute', 0)))
+                    human_sched = f"Daily at {int(getattr(t,'hour',2)):02d}:{int(getattr(t,'minute',0)):02d}"
+                elif schedule_mode == "Weekly":
+                    t = st.session_state.get("sync_time_weekly", dt_time(2, 0))
+                    days = st.session_state.get("sync_weekdays", ["Mon"]) or ["Mon"]
+                    dow_map = {"Mon": "mon", "Tue": "tue", "Wed": "wed", "Thu": "thu", "Fri": "fri", "Sat": "sat", "Sun": "sun"}
+                    day_str = ",".join([dow_map.get(d, "mon") for d in days])
+                    trigger = CronTrigger(day_of_week=day_str, hour=int(getattr(t, 'hour', 2)), minute=int(getattr(t, 'minute', 0)))
+                    human_sched = f"Weekly on {', '.join(days)} at {int(getattr(t,'hour',2)):02d}:{int(getattr(t,'minute',0)):02d}"
+                elif schedule_mode == "Monthly":
+                    t = st.session_state.get("sync_time_monthly", dt_time(2, 0))
+                    dom = int(st.session_state.get("sync_dom", 1))
+                    trigger = CronTrigger(day=dom, hour=int(getattr(t, 'hour', 2)), minute=int(getattr(t, 'minute', 0)))
+                    human_sched = f"Monthly on day {dom} at {int(getattr(t,'hour',2)):02d}:{int(getattr(t,'minute',0)):02d}"
+                elif schedule_mode == "Every X minutes":
+                    trigger = IntervalTrigger(minutes=int(interval_minutes))
+                    human_sched = f"Every {int(interval_minutes)} minute(s)"
+                else:
+                    trigger = CronTrigger.from_crontab(str(cron_expr))
+                    human_sched = f"Cron: {cron_expr}"
+                # Implied SLA fixed to 5 minutes
+                implied_sla_min = 5
+
+                in_dir = sync_folder.strip()
+                if not in_dir:
+                    st.error("Please provide a folder to sync")
+                else:
+                    base = Path(in_dir).name or "folder"
+                    jid = f"sync::{hashlib.sha1(in_dir.encode('utf-8')).hexdigest()[:8]}::{base}"
+                    jname = f"{base} ‚Äî {human_sched}"
+                    ensure_job(jid, jname, in_dir, md_out_target or None, implied_sla_min)
+                    sched.add_job(_job_ingest_fn, id=jid, name=jname, args=[in_dir, md_out_target or None, jid, jname], trigger=trigger, replace_existing=True)
+                    st.success("Continuous sync enabled")
+            except Exception as e:
+                st.error(f"Failed to start: {e}")
 
-    # Redesigned scheduler: single add-job form + run-now
-    with st.form("add_job_form", clear_on_submit=False):
-        col_a, col_b = st.columns([3, 2])
-        with col_a:
-            sync_folder = st.text_input("Folder to sync", key="sync_folder", placeholder="/path/to/folder")
-            if sync_folder and not os.path.isdir(sync_folder):
-                st.caption("Invalid folder path")
-            md_out_target = st.text_input("Markdown output dir (optional)", key="sync_md_out")
-            if md_out_target and not os.path.isdir(md_out_target):
-                st.caption("Invalid folder path")
-        with col_b:
-            schedule_mode = st.selectbox("Schedule", options=["Daily", "Every X minutes", "Weekly", "Monthly", "Cron (advanced)"], key="sync_mode")
-            interval_minutes = int(st.number_input("Interval (minutes)", min_value=1, step=1, key="sync_interval")) if schedule_mode == "Every X minutes" else 1440
-            cron_expr = st.text_input("Cron (min hr dom mon dow)", key="sync_cron") if schedule_mode == "Cron (advanced)" else st.session_state.get("sync_cron", "0 2 * * *")
-            if schedule_mode == "Daily":
-                st.time_input("Time of day", key="sync_time")
-            elif schedule_mode == "Weekly":
-                days_options = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
-                st.multiselect("Days of week", options=days_options, key="sync_weekdays")
-                st.time_input("Time of day", key="sync_time_weekly")
-            elif schedule_mode == "Monthly":
-                st.number_input("Day of month", min_value=1, max_value=31, step=1, key="sync_dom")
-                st.time_input("Time of day", key="sync_time_monthly")
-        col1, col2 = st.columns([1,1])
-        add_job_submit = col1.form_submit_button("Add job", use_container_width=True)
-        run_now_submit = col2.form_submit_button("Run now", use_container_width=True)
-
-    from apscheduler.schedulers.background import BackgroundScheduler
-    # ThreadPoolExecutor not needed for interface-only scheduler
-    from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
-    from apscheduler.triggers.interval import IntervalTrigger
-    from apscheduler.triggers.cron import CronTrigger
-    from apscheduler.triggers.date import DateTrigger
-    from resume_ui.scheduler_cli import _job_ingest as _job_ingest_fn
-    from kb.db import ensure_job, fetch_recent_runs, request_cancel_run
-
-    # Scheduler for UI (add/remove only, no execution)
-    def _get_scheduler_interface():
-        jobstores = {'default': SQLAlchemyJobStore(url=get_database_url())}
-        # No executors needed if we don't start it, but good to have defaults
-        job_defaults = {'coalesce': True, 'max_instances': 1, 'misfire_grace_time': 300}
-        return BackgroundScheduler(jobstores=jobstores, job_defaults=job_defaults)
-
-    job_id = "ui_continuous_sync"
-    if add_job_submit:
-        try:
-            # We do NOT start the scheduler in the UI process.
-            # We just add the job to the store; the external scheduler service picks it up.
-            sched = _get_scheduler_interface()
-            trigger = None
-            if schedule_mode == "Daily":
-                t = st.session_state.get("sync_time", dt_time(2, 0))
-                trigger = CronTrigger(hour=int(getattr(t, 'hour', 2)), minute=int(getattr(t, 'minute', 0)))
-                human_sched = f"Daily at {int(getattr(t,'hour',2)):02d}:{int(getattr(t,'minute',0)):02d}"
-            elif schedule_mode == "Weekly":
-                t = st.session_state.get("sync_time_weekly", dt_time(2, 0))
-                days = st.session_state.get("sync_weekdays", ["Mon"]) or ["Mon"]
-                dow_map = {"Mon": "mon", "Tue": "tue", "Wed": "wed", "Thu": "thu", "Fri": "fri", "Sat": "sat", "Sun": "sun"}
-                day_str = ",".join([dow_map.get(d, "mon") for d in days])
-                trigger = CronTrigger(day_of_week=day_str, hour=int(getattr(t, 'hour', 2)), minute=int(getattr(t, 'minute', 0)))
-                human_sched = f"Weekly on {', '.join(days)} at {int(getattr(t,'hour',2)):02d}:{int(getattr(t,'minute',0)):02d}"
-            elif schedule_mode == "Monthly":
-                t = st.session_state.get("sync_time_monthly", dt_time(2, 0))
-                dom = int(st.session_state.get("sync_dom", 1))
-                trigger = CronTrigger(day=dom, hour=int(getattr(t, 'hour', 2)), minute=int(getattr(t, 'minute', 0)))
-                human_sched = f"Monthly on day {dom} at {int(getattr(t,'hour',2)):02d}:{int(getattr(t,'minute',0)):02d}"
-            elif schedule_mode == "Every X minutes":
-                trigger = IntervalTrigger(minutes=int(interval_minutes))
-                human_sched = f"Every {int(interval_minutes)} minute(s)"
-            else:
-                trigger = CronTrigger.from_crontab(str(cron_expr))
-                human_sched = f"Cron: {cron_expr}"
-            # Implied SLA fixed to 5 minutes
-            implied_sla_min = 5
-
-            in_dir = sync_folder.strip()
-            if not in_dir:
-                st.error("Please provide a folder to sync")
-            else:
-                base = Path(in_dir).name or "folder"
-                jid = f"sync::{hashlib.sha1(in_dir.encode('utf-8')).hexdigest()[:8]}::{base}"
-                jname = f"{base} ‚Äî {human_sched}"
-                ensure_job(jid, jname, in_dir, md_out_target or None, implied_sla_min)
-                sched.add_job(_job_ingest_fn, id=jid, name=jname, args=[in_dir, md_out_target or None, jid, jname], trigger=trigger, replace_existing=True)
-                st.success("Continuous sync enabled")
-        except Exception as e:
-            st.error(f"Failed to start: {e}")
+        if run_now_submit:
+            try:
+                in_dir = sync_folder.strip()
+                if not in_dir or not os.path.isdir(in_dir):
+                    st.error("Please provide a valid folder to sync")
+                else:
+                    sched = _get_scheduler_interface()
+                    jid = f"run::{hashlib.sha1((in_dir + str(uuid.uuid4())).encode('utf-8')).hexdigest()[:8]}"
+                    # For one-off runs, SLA is not applicable; store None
+                    ensure_job(jid, f"One-off run ‚Äî {Path(in_dir).name}", in_dir, md_out_target or None, None)
+                    sched.add_job(_job_ingest_fn, id=jid, args=[in_dir, md_out_target or None, jid, f"One-off run ‚Äî {Path(in_dir).name}"], trigger=DateTrigger(run_date=datetime.now(timezone.utc) + timedelta(seconds=1)))
+                    st.success("Queued run in background")
+            except Exception as e:
+                st.error(f"Run failed: {e}")
 
-    if run_now_submit:
+        # Always show current jobs with human-friendly labels and selection, wrapped in a form to prevent flicker
         try:
-            in_dir = sync_folder.strip()
-            if not in_dir or not os.path.isdir(in_dir):
-                st.error("Please provide a valid folder to sync")
+            sched = _get_scheduler_interface()
+            jobs = sched.get_jobs()
+            if not jobs:
+                st.info("No scheduled jobs")
             else:
-                sched = _get_scheduler_interface()
-                jid = f"run::{hashlib.sha1((in_dir + str(uuid.uuid4())).encode('utf-8')).hexdigest()[:8]}"
-                # For one-off runs, SLA is not applicable; store None
-                ensure_job(jid, f"One-off run ‚Äî {Path(in_dir).name}", in_dir, md_out_target or None, None)
-                sched.add_job(_job_ingest_fn, id=jid, args=[in_dir, md_out_target or None, jid, f"One-off run ‚Äî {Path(in_dir).name}"], trigger=DateTrigger(run_date=datetime.now(timezone.utc) + timedelta(seconds=1)))
-                st.success("Queued run in background")
-        except Exception as e:
-            st.error(f"Run failed: {e}")
-
-    # Always show current jobs with human-friendly labels and selection, wrapped in a form to prevent flicker
-    try:
-        sched = _get_scheduler_interface()
-        jobs = sched.get_jobs()
-        if not jobs:
-            st.info("No scheduled jobs")
-        else:
-            labels = {}
-            options = []
-            for j in jobs:
-                folder = None
-                try:
-                    if j.args and len(j.args) >= 1 and isinstance(j.args[0], str):
-                        folder = Path(j.args[0]).name
-                except Exception:
+                labels = {}
+                options = []
+                for j in jobs:
                     folder = None
-                label = j.name or f"{folder or j.id} ‚Äî {str(j.trigger)}"
-                if j.next_run_time:
-                    label = f"{label} ‚Äî next {j.next_run_time}"
-                labels[j.id] = label
-                options.append(j.id)
-            st.caption("Scheduled jobs")
-            with st.form("jobs_form", clear_on_submit=False):
-                selected_jobs = st.multiselect("Select jobs", options=options, format_func=lambda jid: labels.get(jid, jid), key="ui_jobs_multi")
-                col_rm, col_run_sel = st.columns([1,1])
-                rm_pressed = col_rm.form_submit_button("Remove selected", disabled=(not selected_jobs), use_container_width=True)
-                run_pressed = col_run_sel.form_submit_button("Run selected now", disabled=(not selected_jobs), use_container_width=True)
-                purge_pressed = st.form_submit_button("Purge all jobs", use_container_width=True)
-                if rm_pressed:
-                    removed = 0
-                    for jid in list(selected_jobs):
-                        try:
-                            sched.remove_job(jid)
-                            removed += 1
-                        except Exception:
-                            pass
-                    if removed > 0:
-                        st.success(f"Removed {removed} job(s)")
-                    else:
-                        st.info("No jobs removed")
-                if run_pressed:
-                    queued = 0
-                    for jid in list(selected_jobs):
-                        try:
-                            j = sched.get_job(jid)
-                            if j and j.args and isinstance(j.args[0], str):
-                                in_dir = j.args[0]
-                                out_dir = j.args[1] if len(j.args) > 1 else None
-                                rqid = f"run::{hashlib.sha1((in_dir + str(uuid.uuid4())).encode('utf-8')).hexdigest()[:8]}"
-                                ensure_job(rqid, f"One-off run ‚Äî {Path(in_dir).name}", in_dir, out_dir)
-                                sched.add_job(_job_ingest_fn, id=rqid, args=[in_dir, out_dir, rqid, f"One-off run ‚Äî {Path(in_dir).name}"], trigger=DateTrigger(run_date=datetime.now(timezone.utc) + timedelta(seconds=1)))
-                                queued += 1
-                        except Exception:
-                            pass
-                    if queued > 0:
-                        st.success(f"Queued {queued} job(s)")
-                    else:
-                        st.info("No jobs queued")
-                if purge_pressed:
                     try:
-                        for jid in list(options):
+                        if j.args and len(j.args) >= 1 and isinstance(j.args[0], str):
+                            folder = Path(j.args[0]).name
+                    except Exception:
+                        folder = None
+                    label = j.name or f"{folder or j.id} ‚Äî {str(j.trigger)}"
+                    if j.next_run_time:
+                        label = f"{label} ‚Äî next {j.next_run_time}"
+                    labels[j.id] = label
+                    options.append(j.id)
+                st.caption("Scheduled jobs")
+                with st.form("jobs_form", clear_on_submit=False):
+                    selected_jobs = st.multiselect("Select jobs", options=options, format_func=lambda jid: labels.get(jid, jid), key="ui_jobs_multi")
+                    col_rm, col_run_sel = st.columns([1,1])
+                    rm_pressed = col_rm.form_submit_button("Remove selected", disabled=(not selected_jobs), use_container_width=True)
+                    run_pressed = col_run_sel.form_submit_button("Run selected now", disabled=(not selected_jobs), use_container_width=True)
+                    purge_pressed = st.form_submit_button("Purge all jobs", use_container_width=True)
+                    if rm_pressed:
+                        removed = 0
+                        for jid in list(selected_jobs):
                             try:
                                 sched.remove_job(jid)
+                                removed += 1
                             except Exception:
                                 pass
-                        st.success("Purged all jobs")
-                    except Exception as e:
-                        st.error(f"Failed to purge: {e}")
-    except Exception as e:
-        st.error(f"Failed to list/remove jobs: {e}")
-
-    # Removed single remove button; handled via multiselect above
-
-    st.divider()
-    st.subheader("Recent runs")
-    if st.button("Refresh", use_container_width=False):
-        st.rerun()
-    try:
-        runs = fetch_recent_runs(limit=20)
-        if not runs:
-            st.caption("No runs yet")
-        else:
-            for rid, jid, in_dir, out_dir, status, progress, processed, total, chunks, started_at, finished_at, last_msg, log_tail, cancel_req, err in runs:
-                with st.container():
-                    st.markdown(f"**{status.upper()}** ‚Äî {progress}% ‚Äî {Path(in_dir).name}  ")
-                    st.caption(f"Run {rid} ‚Äî job {jid or '-'} ‚Äî started {started_at}{' ‚Äî finished ' + str(finished_at) if finished_at else ''}")
-                    cols = st.columns([3,1,1])
-                    with cols[0]:
-                        st.progress(int(progress or 0), text=last_msg or "")
-                        if log_tail:
-                            with st.expander("View recent log"):
-                                st.code(str(log_tail))
-                    with cols[1]:
-                        if status in ("running", "queued") and not cancel_req:
-                            if st.button("Cancel", key=f"cancel_{rid}", use_container_width=True):
+                        if removed > 0:
+                            st.success(f"Removed {removed} job(s)")
+                        else:
+                            st.info("No jobs removed")
+                    if run_pressed:
+                        queued = 0
+                        for jid in list(selected_jobs):
+                            try:
+                                j = sched.get_job(jid)
+                                if j and j.args and isinstance(j.args[0], str):
+                                    in_dir = j.args[0]
+                                    out_dir = j.args[1] if len(j.args) > 1 else None
+                                    rqid = f"run::{hashlib.sha1((in_dir + str(uuid.uuid4())).encode('utf-8')).hexdigest()[:8]}"
+                                    ensure_job(rqid, f"One-off run ‚Äî {Path(in_dir).name}", in_dir, out_dir)
+                                    sched.add_job(_job_ingest_fn, id=rqid, args=[in_dir, out_dir, rqid, f"One-off run ‚Äî {Path(in_dir).name}"], trigger=DateTrigger(run_date=datetime.now(timezone.utc) + timedelta(seconds=1)))
+                                    queued += 1
+                            except Exception:
+                                pass
+                        if queued > 0:
+                            st.success(f"Queued {queued} job(s)")
+                        else:
+                            st.info("No jobs queued")
+                    if purge_pressed:
+                        try:
+                            for jid in list(options):
                                 try:
-                                    request_cancel_run(rid)
-                                    st.success("Cancellation requested")
-                                except Exception as e:
-                                    st.error(f"Cancel failed: {e}")
-                    with cols[2]:
-                        st.caption(f"Files {processed}/{total}")
-                        if chunks is not None:
-                            st.caption(f"Chunks {int(chunks)}")
-                        if err:
-                            st.caption(f"Error: {err}")
-    except Exception as e:
-        st.error(f"Failed to load runs: {e}")
-    st.subheader("Edit and delete knowledge")
+                                    sched.remove_job(jid)
+                                except Exception:
+                                    pass
+                            st.success("Purged all jobs")
+                        except Exception as e:
+                            st.error(f"Failed to purge: {e}")
+        except Exception as e:
+            st.error(f"Failed to list/remove jobs: {e}")
 
-    def _friendly_title(path: str, chunk_name: str, content: str) -> str:
-        # Prefer first Markdown heading if present
-        try:
-            for line in content.splitlines():
-                s = line.strip()
-                if s.startswith("# "):
-                    return s[2:].strip()
-                if s.startswith("## "):
-                    return s[3:].strip()
-                if s and not s.startswith("#") and len(s) > 8:
-                    # fallback: first non-empty line with some length
-                    return s[:80]
-        except Exception:
-            pass
-        # Fallback to filename without noisy tokens
-        name = Path(path).name
-        base = name.rsplit(".", 1)[0]
-        cleaned = base.replace("_", " ").replace("-", " ")
-        cleaned = " ".join(part for part in cleaned.split() if not part.isupper() or len(part) <= 5)
-        return cleaned.strip() or chunk_name or name
-
-    # Controls
-    colf, colp, colr = st.columns([4, 1, 1])
-    with colf:
-        filter_text = st.text_input("Filter (path, name, or content)", key="kb_m_filter")
-    with colp:
-        rows_per_page = st.number_input("Rows/page", min_value=5, max_value=200, step=5, key="kb_m_rpp")
-    with colr:
-        if "kb_m_page" not in st.session_state:
-            st.session_state["kb_m_page"] = 1
-        page = st.number_input("Page", min_value=1, value=int(st.session_state["kb_m_page"]), step=1)
-        st.session_state["kb_m_page"] = int(page)
-
-    like = f"%{st.session_state.get('kb_m_filter','')}%" if st.session_state.get('kb_m_filter') else None
-    total = count_chunks(engine, like)
-    max_page = max(1, (total + int(rows_per_page) - 1) // int(rows_per_page))
-    st.caption(f"{total} item(s) ‚Äî page {st.session_state['kb_m_page']} of {max_page}")
-    if st.session_state["kb_m_page"] > max_page:
-        st.session_state["kb_m_page"] = max_page
-
-    offset = (st.session_state["kb_m_page"] - 1) * int(rows_per_page)
-    rows = fetch_chunks(engine, limit=int(rows_per_page), offset=int(offset), like=like)
-
-    # Selection state
-    if "kb_m_selected" not in st.session_state:
-        st.session_state["kb_m_selected"] = []
-
-    # Build dropdown options for this page (id -> label)
-    page_options = []
-    for rid, path, cname, content in rows:
-        title = _friendly_title(path, cname, content)
-        label = f"{title} ‚Äî {Path(path).name} :: {cname}"
-        page_options.append((rid, label))
-    option_labels = {rid: label for rid, label in page_options}
-
-    # Searchable multiselect (limited to current page for performance), wrapped in a form to reduce reruns
-    with st.form("kb_manage_form", clear_on_submit=False):
-        selected_ids = st.multiselect(
-            "Select items (current page)",
-            options=[rid for rid, _ in page_options],
-            default=[rid for rid in st.session_state.get("kb_m_selected", []) if any(rid == rid2 for rid2, _ in page_options)],
-            format_func=lambda rid: option_labels.get(rid, str(rid)),
-            key="kb_m_multi",
-        )
-        submit_del = st.form_submit_button("Apply selection")
-    st.session_state["kb_m_selected"] = list(selected_ids)
-
-    # Auto preview first selected, with prev/next controls if multiple
-    if st.session_state["kb_m_selected"]:
-        sel_ids = st.session_state["kb_m_selected"]
-        idx = st.session_state.get("kb_m_prev_idx", 0)
-        idx = max(0, min(idx, len(sel_ids) - 1))
-        cols_nav = st.columns([1, 3, 1])
-        with cols_nav[0]:
-            if st.button("‚óÄ Prev", disabled=(idx <= 0)):
-                st.session_state["kb_m_prev_idx"] = max(0, idx - 1)
-                st.rerun()
-        with cols_nav[2]:
-            if st.button("Next ‚ñ∂", disabled=(idx >= len(sel_ids) - 1)):
-                st.session_state["kb_m_prev_idx"] = min(len(sel_ids) - 1, idx + 1)
-                st.rerun()
+        # Removed single remove button; handled via multiselect above
 
-        current_id = sel_ids[idx]
-        row = fetch_chunk_by_id(engine, int(current_id))
-        if row:
-            _rid, path, cname, content = row
-            st.caption(f"{path} :: {cname}")
-            st.text_area("Content", content, height=360)
-
-    del_count = len(st.session_state["kb_m_selected"])
-    with st.form("kb_delete_form", clear_on_submit=False):
-        btn_del = st.form_submit_button(f"Delete selected ({del_count})", disabled=(del_count == 0))
-    if btn_del:
-        try:
-            n = delete_chunks_by_ids(engine, list(st.session_state["kb_m_selected"]))
-            st.success(f"Deleted {n} item(s)")
-            st.session_state["kb_m_selected"] = []
-            st.session_state.pop("kb_m_prev_idx", None)
-            # Invalidate search cache
-            st.session_state.pop("kb_searcher", None)
-            st.session_state.pop("kb_searcher_docs_len", None)
+        st.subheader("Recent runs")
+        if st.button("Refresh", use_container_width=False):
             st.rerun()
+        try:
+            runs = fetch_recent_runs(limit=20)
+            if not runs:
+                st.caption("No runs yet")
+            else:
+                for rid, jid, in_dir, out_dir, status, progress, processed, total, chunks, started_at, finished_at, last_msg, log_tail, cancel_req, err in runs:
+                    with st.container():
+                        st.markdown(f"**{status.upper()}** ‚Äî {progress}% ‚Äî {Path(in_dir).name}  ")
+                        st.caption(f"Run {rid} ‚Äî job {jid or '-'} ‚Äî started {started_at}{' ‚Äî finished ' + str(finished_at) if finished_at else ''}")
+                        cols = st.columns([3,1,1])
+                        with cols[0]:
+                            st.progress(int(progress or 0), text=last_msg or "")
+                            if log_tail:
+                                with st.expander("View recent log"):
+                                    st.code(str(log_tail))
+                        with cols[1]:
+                            if status in ("running", "queued") and not cancel_req:
+                                if st.button("Cancel", key=f"cancel_{rid}", use_container_width=True):
+                                    try:
+                                        request_cancel_run(rid)
+                                        st.success("Cancellation requested")
+                                    except Exception as e:
+                                        st.error(f"Cancel failed: {e}")
+                        with cols[2]:
+                            st.caption(f"Files {processed}/{total}")
+                            if chunks is not None:
+                                st.caption(f"Chunks {int(chunks)}")
+                            if err:
+                                st.caption(f"Error: {err}")
         except Exception as e:
-            st.error(f"Delete failed: {e}")
+            st.error(f"Failed to load runs: {e}")
+    
+    with tab_browse:
+        st.subheader("Edit and delete knowledge")
 
-    st.divider()
-    st.subheader("Danger zone")
-    st.caption("Factory reset DB: wipes jobs and KB tables. This cannot be undone.")
-    with st.form("factory_reset_form", clear_on_submit=False):
-        confirm = st.text_input("Type RESET to confirm", key="factory_reset_confirm")
-        do_reset = st.form_submit_button("Factory reset DB")
-    if do_reset:
-        if (st.session_state.get("factory_reset_confirm", "").strip().upper() != "RESET"):
-            st.error("Please type RESET to confirm.")
-        else:
+        def _friendly_title(path: str, chunk_name: str, content: str) -> str:
+            # Prefer first Markdown heading if present
             try:
-                from kb.db import factory_reset_db
-                factory_reset_db()
-                st.success("Database reset complete")
+                for line in content.splitlines():
+                    s = line.strip()
+                    if s.startswith("# "):
+                        return s[2:].strip()
+                    if s.startswith("## "):
+                        return s[3:].strip()
+                    if s and not s.startswith("#") and len(s) > 8:
+                        # fallback: first non-empty line with some length
+                        return s[:80]
+            except Exception:
+                pass
+            # Fallback to filename without noisy tokens
+            name = Path(path).name
+            base = name.rsplit(".", 1)[0]
+            cleaned = base.replace("_", " ").replace("-", " ")
+            cleaned = " ".join(part for part in cleaned.split() if not part.isupper() or len(part) <= 5)
+            return cleaned.strip() or chunk_name or name
+
+        # Controls
+        colf, colp, colr = st.columns([4, 1, 1])
+        with colf:
+            filter_text = st.text_input("Filter (path, name, or content)", key="kb_m_filter")
+        with colp:
+            rows_per_page = st.number_input("Rows/page", min_value=5, max_value=200, step=5, key="kb_m_rpp")
+        with colr:
+            if "kb_m_page" not in st.session_state:
+                st.session_state["kb_m_page"] = 1
+            page = st.number_input("Page", min_value=1, value=int(st.session_state["kb_m_page"]), step=1)
+            st.session_state["kb_m_page"] = int(page)
+
+        like = f"%{st.session_state.get('kb_m_filter','')}%" if st.session_state.get('kb_m_filter') else None
+        total = count_chunks(engine, like)
+        max_page = max(1, (total + int(rows_per_page) - 1) // int(rows_per_page))
+        st.caption(f"{total} item(s) ‚Äî page {st.session_state['kb_m_page']} of {max_page}")
+        if st.session_state["kb_m_page"] > max_page:
+            st.session_state["kb_m_page"] = max_page
+
+        offset = (st.session_state["kb_m_page"] - 1) * int(rows_per_page)
+        rows = fetch_chunks(engine, limit=int(rows_per_page), offset=int(offset), like=like)
+
+        # Selection state
+        if "kb_m_selected" not in st.session_state:
+            st.session_state["kb_m_selected"] = []
+
+        # Build dropdown options for this page (id -> label)
+        page_options = []
+        for rid, path, cname, content in rows:
+            title = _friendly_title(path, cname, content)
+            label = f"{title} ‚Äî {Path(path).name} :: {cname}"
+            page_options.append((rid, label))
+        option_labels = {rid: label for rid, label in page_options}
+
+        # Searchable multiselect (limited to current page for performance), wrapped in a form to reduce reruns
+        with st.form("kb_manage_form", clear_on_submit=False):
+            selected_ids = st.multiselect(
+                "Select items (current page)",
+                options=[rid for rid, _ in page_options],
+                default=[rid for rid in st.session_state.get("kb_m_selected", []) if any(rid == rid2 for rid2, _ in page_options)],
+                format_func=lambda rid: option_labels.get(rid, str(rid)),
+                key="kb_m_multi",
+            )
+            submit_del = st.form_submit_button("Apply selection")
+        st.session_state["kb_m_selected"] = list(selected_ids)
+
+        # Auto preview first selected, with prev/next controls if multiple
+        if st.session_state["kb_m_selected"]:
+            sel_ids = st.session_state["kb_m_selected"]
+            idx = st.session_state.get("kb_m_prev_idx", 0)
+            idx = max(0, min(idx, len(sel_ids) - 1))
+            cols_nav = st.columns([1, 3, 1])
+            with cols_nav[0]:
+                if st.button("‚óÄ Prev", disabled=(idx <= 0)):
+                    st.session_state["kb_m_prev_idx"] = max(0, idx - 1)
+                    st.rerun()
+            with cols_nav[2]:
+                if st.button("Next ‚ñ∂", disabled=(idx >= len(sel_ids) - 1)):
+                    st.session_state["kb_m_prev_idx"] = min(len(sel_ids) - 1, idx + 1)
+                    st.rerun()
+
+            current_id = sel_ids[idx]
+            row = fetch_chunk_by_id(engine, int(current_id))
+            if row:
+                _rid, path, cname, content = row
+                st.caption(f"{path} :: {cname}")
+                st.text_area("Content", content, height=360)
+
+        del_count = len(st.session_state["kb_m_selected"])
+        with st.form("kb_delete_form", clear_on_submit=False):
+            btn_del = st.form_submit_button(f"Delete selected ({del_count})", disabled=(del_count == 0))
+        if btn_del:
+            try:
+                n = delete_chunks_by_ids(engine, list(st.session_state["kb_m_selected"]))
+                st.success(f"Deleted {n} item(s)")
+                st.session_state["kb_m_selected"] = []
+                st.session_state.pop("kb_m_prev_idx", None)
+                # Invalidate search cache
+                st.session_state.pop("kb_searcher", None)
+                st.session_state.pop("kb_searcher_docs_len", None)
+                st.rerun()
             except Exception as e:
-                st.error(f"Reset failed: {e}")
+                st.error(f"Delete failed: {e}")
+
+    with tab_sys:
+        st.subheader("Danger zone")
+        st.caption("Factory reset DB: wipes jobs and KB tables. This cannot be undone.")
+        with st.form("factory_reset_form", clear_on_submit=False):
+            confirm = st.text_input("Type RESET to confirm", key="factory_reset_confirm")
+            do_reset = st.form_submit_button("Factory reset DB")
+        if do_reset:
+            if (st.session_state.get("factory_reset_confirm", "").strip().upper() != "RESET"):
+                st.error("Please type RESET to confirm.")
+            else:
+                try:
+                    from kb.db import factory_reset_db
+                    factory_reset_db()
+                    st.success("Database reset complete")
+                except Exception as e:
+                    st.error(f"Reset failed: {e}")
